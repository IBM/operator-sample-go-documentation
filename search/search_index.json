{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Operator sample go documentation \u00b6 \"Operator Sample Go\" project \u00b6 The Operator Sample Go project contains Kubernetes operator samples that demonstrate best practices of how to develop operators with Golang using Operator SDK including Kubebuilder and the Operator Lifecycle Manager Framework . The sample operators in this project go far beyond a typical operator getting started tutorial, providing a useful reference implementation for typical enterprise applications. Fast start on YouTube \u00b6 Overview Short demo More detailed overview and demo Credits \u00b6 Niklas Heidloff Alain Airom Adam de Leeuw Diwakar Tiwari Thomas S\u00fcdbr\u00f6cker Vishal Ramani","title":"Operator sample go documentation"},{"location":"#operator-sample-go-documentation","text":"","title":"Operator sample go documentation"},{"location":"#operator-sample-go-project","text":"The Operator Sample Go project contains Kubernetes operator samples that demonstrate best practices of how to develop operators with Golang using Operator SDK including Kubebuilder and the Operator Lifecycle Manager Framework . The sample operators in this project go far beyond a typical operator getting started tutorial, providing a useful reference implementation for typical enterprise applications.","title":"\"Operator Sample Go\" project"},{"location":"#fast-start-on-youtube","text":"Overview Short demo More detailed overview and demo","title":"Fast start on YouTube"},{"location":"#credits","text":"Niklas Heidloff Alain Airom Adam de Leeuw Diwakar Tiwari Thomas S\u00fcdbr\u00f6cker Vishal Ramani","title":"Credits"},{"location":"advanced-capabilities-overview/","text":"Advanced Capabilities \u00b6 Exporting Metrics from Kubernetes Apps for Prometheus Accessing Kubernetes from Go Applications How to build your own Database on Kubernetes Building Databases on Kubernetes with Quarkus","title":"7.1 Overview"},{"location":"advanced-capabilities-overview/#advanced-capabilities","text":"Exporting Metrics from Kubernetes Apps for Prometheus Accessing Kubernetes from Go Applications How to build your own Database on Kubernetes Building Databases on Kubernetes with Quarkus","title":"Advanced Capabilities"},{"location":"automation-overview/","text":"Automation with Scripts \u00b6 Automation scripts are the recommended way to deploy the sample operators. The scripts provide the following functions: Verify workstation prerequisites Install prerequisite components to a Kubernetes or OpenShift cluster Create a 'demo environment' which deploys working golden images to a Kubernetes or OpenShift cluster Continous integration deployment which builds all operators & applications, pushes to a registry and deploys the images using OLM to either Kubernetes or OpenShift Required Setup \u00b6 Ensure you have followed the steps in the prerequisites section If you created a version_local.env file because you plan to build new container images, open a terminal window and use the versions_local.env as input for your environment variables: source versions_local.env podman login $REGISTRY Script automation process \u00b6 The script automation does following: The functionality has variations depending on the script you are going to use. It ensures that with two versions.env files the tagging for the container images works in a consistent way. versions.env for golden sources versions_local.env for custom local configurations your version of versions_local.env-template It creates a temp github tag related to the last commit before the automation was started. It creates various logs script-automation.log these files which will not be loaded to the git repo. Resets the cluster environment: OLM installation Cert manager installation Prometheus operator installation Clean the installed operators Creates following containers images: Database operator related Database-service operator-database operator-database-backup operator-database-bundle , this is a container image which will be created by the operator-sdk and will be used later inside the operator-database-catalog which is relevant for OLM usage. operator-database-catalog , this container image contains a reference to the operator-database-bundle and will be used in the context of OLM Application operator related simple-microservice operator-application-autoscaler operator-application operator-application-bundle , that is a container image which will be created by the operator-sdk and will be used later inside the operator-application-catalog which is relevant for OLM usage. operator-application-catalog , that container image contains a reference to the operator-application-bundle and will be used in the context of OLM It uses templates to generate OLM deployment resources. The resources in the repo under folders 'olm' are not used by the script. operator-application templates operator-database templates Resets the podman vm if needed and starts podman. It creates role.yaml , role-binding.yaml , clusterserviceversion.yaml and sample custom resources for the given operators, based on templates. It verifies the pre-requisites. Setup of the needed bin directory of the operator-sdk projects. ( controller-gen , kustomize , opm , setup-envtest )","title":"4.1 Overview"},{"location":"automation-overview/#automation-with-scripts","text":"Automation scripts are the recommended way to deploy the sample operators. The scripts provide the following functions: Verify workstation prerequisites Install prerequisite components to a Kubernetes or OpenShift cluster Create a 'demo environment' which deploys working golden images to a Kubernetes or OpenShift cluster Continous integration deployment which builds all operators & applications, pushes to a registry and deploys the images using OLM to either Kubernetes or OpenShift","title":"Automation with Scripts"},{"location":"automation-overview/#required-setup","text":"Ensure you have followed the steps in the prerequisites section If you created a version_local.env file because you plan to build new container images, open a terminal window and use the versions_local.env as input for your environment variables: source versions_local.env podman login $REGISTRY","title":"Required Setup"},{"location":"automation-overview/#script-automation-process","text":"The script automation does following: The functionality has variations depending on the script you are going to use. It ensures that with two versions.env files the tagging for the container images works in a consistent way. versions.env for golden sources versions_local.env for custom local configurations your version of versions_local.env-template It creates a temp github tag related to the last commit before the automation was started. It creates various logs script-automation.log these files which will not be loaded to the git repo. Resets the cluster environment: OLM installation Cert manager installation Prometheus operator installation Clean the installed operators Creates following containers images: Database operator related Database-service operator-database operator-database-backup operator-database-bundle , this is a container image which will be created by the operator-sdk and will be used later inside the operator-database-catalog which is relevant for OLM usage. operator-database-catalog , this container image contains a reference to the operator-database-bundle and will be used in the context of OLM Application operator related simple-microservice operator-application-autoscaler operator-application operator-application-bundle , that is a container image which will be created by the operator-sdk and will be used later inside the operator-application-catalog which is relevant for OLM usage. operator-application-catalog , that container image contains a reference to the operator-application-bundle and will be used in the context of OLM It uses templates to generate OLM deployment resources. The resources in the repo under folders 'olm' are not used by the script. operator-application templates operator-database templates Resets the podman vm if needed and starts podman. It creates role.yaml , role-binding.yaml , clusterserviceversion.yaml and sample custom resources for the given operators, based on templates. It verifies the pre-requisites. Setup of the needed bin directory of the operator-sdk projects. ( controller-gen , kustomize , opm , setup-envtest )","title":"Script automation process"},{"location":"automation-parameter-reference/","text":"Script Parameter Reference \u00b6 Reference Information \u00b6 There are four types of installation scripts: install-required demo ci delete 1. install-required -xxx-components.sh \u00b6 Definition: xxx == Kubernetes or OpenShift Table overview: Installs the required components for Kubernetes or OpenShift. Component Kubernetes OpenShift CertManager Yes Yes OLM Yes No Prometheus Operator Yes No Prometheus Instance Yes Only configuration 2. demo -xxx-yyyy.sh \u00b6 Setup or delete based on the golden source versions (version.env). Definition: yyy == Kubernetes or OpenShift xxx == delete or setup Table overview: Name Kubernetes OpenShift Parameters demo -kubernetes-operators.sh Yes No app demo reset demo -kubernetes-operator-application.sh Yes No demo reset demo -kubernetes-operator-database.sh Yes No demo demo -opershift-operators.sh No Yes app demo reset demo -opershift-operator-application.sh No Yes demo reset demo -opershift-operator-database.sh No Yes demo Setup demo for Kubernetes sh scripts/demo-kubernetes-operators.sh app demo reset Setup demo for OpenShift sh scripts/demo-openshift-operators.sh app demo reset 3. ci -www-xxx-yyy-zzz.sh \u00b6 Definition: Creates all operators or specific operators of the project in Kubernetes or OpenShift. www == create or delete xxx == operator or operators zzz == Kubernetes or OpenShift Table overview: Here is a list of available ci scripts. Name Kubernetes OpenShift Creates Database Operator Creates Application Operator ci -create-operator-database- kubernetes .sh Yes No Yes No ci -create-operator-application- kubernetes .sh Yes No No Yes ci -create-operators- kubernetes .sh Yes No Yes Yes ci -create-operator-database- openshift .sh No Yes Yes No ci -create-operator-application- openshift .sh No Yes No Yes ci -create-operators- openshift .sh No Yes Yes Yes 4. delete-everything -xxx \u00b6 Deletes all depending on the Platfrom such as the operators, OLM, Prometheus or Cert-Manager. xxx == Kubernetes or OpenShift \ud83d\udd34 IMPORTANT: The order of the parameters is hard coded! The ci-create-operators-kubernetes.sh & ci-create-operators-openshift.sh scripts supports the following parameter options. Parameter combination versions.env versions_local.env delete all and setup prerequisites creates operator database creates operator application reset podman database local no yes no yes no no database local reset no yes yes yes no no database local reset podman_reset no yes yes yes no yes app local no yes no yes yes no app local reset no yes yes yes yes no app local reset podman_reset no yes yes yes yes yes First parameter: ('database' or 'app') Use 'database' for setup the database operator only Use 'app' for setup the database and application operator. Second parameter: ('local' or 'ci') Use 'local' for using the versions_local.env file as input for the container tags. Use 'ci' for using the versions.env file as input for the container tags. ONLY FOR GOLDEN SOURCE! Third parameter: ('reset') Use 'reset' to deinstall the operators and prereq and reinstall them. Fourth parameter: ('podman_reset') Use 'podman_reset' to delete podman vm, create a new podman vm with size of 15, and start podman.","title":"4.4 Version Reference"},{"location":"automation-parameter-reference/#script-parameter-reference","text":"","title":"Script Parameter Reference"},{"location":"automation-parameter-reference/#reference-information","text":"There are four types of installation scripts: install-required demo ci delete","title":"Reference Information"},{"location":"automation-parameter-reference/#1-install-required-xxx-componentssh","text":"Definition: xxx == Kubernetes or OpenShift Table overview: Installs the required components for Kubernetes or OpenShift. Component Kubernetes OpenShift CertManager Yes Yes OLM Yes No Prometheus Operator Yes No Prometheus Instance Yes Only configuration","title":"1. install-required-xxx-components.sh"},{"location":"automation-parameter-reference/#2-demo-xxx-yyyysh","text":"Setup or delete based on the golden source versions (version.env). Definition: yyy == Kubernetes or OpenShift xxx == delete or setup Table overview: Name Kubernetes OpenShift Parameters demo -kubernetes-operators.sh Yes No app demo reset demo -kubernetes-operator-application.sh Yes No demo reset demo -kubernetes-operator-database.sh Yes No demo demo -opershift-operators.sh No Yes app demo reset demo -opershift-operator-application.sh No Yes demo reset demo -opershift-operator-database.sh No Yes demo Setup demo for Kubernetes sh scripts/demo-kubernetes-operators.sh app demo reset Setup demo for OpenShift sh scripts/demo-openshift-operators.sh app demo reset","title":"2. demo-xxx-yyyy.sh"},{"location":"automation-parameter-reference/#3-ci-www-xxx-yyy-zzzsh","text":"Definition: Creates all operators or specific operators of the project in Kubernetes or OpenShift. www == create or delete xxx == operator or operators zzz == Kubernetes or OpenShift Table overview: Here is a list of available ci scripts. Name Kubernetes OpenShift Creates Database Operator Creates Application Operator ci -create-operator-database- kubernetes .sh Yes No Yes No ci -create-operator-application- kubernetes .sh Yes No No Yes ci -create-operators- kubernetes .sh Yes No Yes Yes ci -create-operator-database- openshift .sh No Yes Yes No ci -create-operator-application- openshift .sh No Yes No Yes ci -create-operators- openshift .sh No Yes Yes Yes","title":"3. ci-www-xxx-yyy-zzz.sh"},{"location":"automation-parameter-reference/#4-delete-everything-xxx","text":"Deletes all depending on the Platfrom such as the operators, OLM, Prometheus or Cert-Manager. xxx == Kubernetes or OpenShift \ud83d\udd34 IMPORTANT: The order of the parameters is hard coded! The ci-create-operators-kubernetes.sh & ci-create-operators-openshift.sh scripts supports the following parameter options. Parameter combination versions.env versions_local.env delete all and setup prerequisites creates operator database creates operator application reset podman database local no yes no yes no no database local reset no yes yes yes no no database local reset podman_reset no yes yes yes no yes app local no yes no yes yes no app local reset no yes yes yes yes no app local reset podman_reset no yes yes yes yes yes First parameter: ('database' or 'app') Use 'database' for setup the database operator only Use 'app' for setup the database and application operator. Second parameter: ('local' or 'ci') Use 'local' for using the versions_local.env file as input for the container tags. Use 'ci' for using the versions.env file as input for the container tags. ONLY FOR GOLDEN SOURCE! Third parameter: ('reset') Use 'reset' to deinstall the operators and prereq and reinstall them. Fourth parameter: ('podman_reset') Use 'podman_reset' to delete podman vm, create a new podman vm with size of 15, and start podman.","title":"4. delete-everything-xxx"},{"location":"automation-script-usage/","text":"Example Script Usage \u00b6 To install required cluster components for Kubernetes: install-required-kubernetes-components.sh To install required cluster components for OpenShift: install-required-openshift-components.sh To set up the a demo environment using 'golden images' on OpenShift: sh scripts/demo-openshift-operators.sh app demo reset To set up the a demo environment using 'golden images' on Kubernetes: sh scripts/demo-kubernetes-operators.sh app demo reset To build all images, push to a registry and deploy to OpenShift: sh scripts/ci-create-operators-openshift.sh app local reset podman_reset To build all images, push to a registry and deploy to Kubernetes: sh scripts/ci-create-operators-kubernetes.sh app local reset podman_reset For further information, refer to the reference documentation of the scripts and their parameters .","title":"4.2 Script Usage"},{"location":"automation-script-usage/#example-script-usage","text":"To install required cluster components for Kubernetes: install-required-kubernetes-components.sh To install required cluster components for OpenShift: install-required-openshift-components.sh To set up the a demo environment using 'golden images' on OpenShift: sh scripts/demo-openshift-operators.sh app demo reset To set up the a demo environment using 'golden images' on Kubernetes: sh scripts/demo-kubernetes-operators.sh app demo reset To build all images, push to a registry and deploy to OpenShift: sh scripts/ci-create-operators-openshift.sh app local reset podman_reset To build all images, push to a registry and deploy to Kubernetes: sh scripts/ci-create-operators-kubernetes.sh app local reset podman_reset For further information, refer to the reference documentation of the scripts and their parameters .","title":"Example Script Usage"},{"location":"automation-version-reference/","text":"Version Reference \u00b6 The following information defines the environments in which the scripts have been tested, and tested versions of the prerequisite components. An explanation of how the scripts work is also provided. Tested workstation environments \u00b6 The scripts have been validated for the following technical environments: OS Version Tested macOS 12.3.1 OK macOS 11.2.3 OK Tested version of tools and frameworks for the prerequsites \u00b6 Tools or frameworks Version/s (G)obal, (L) ocal or (C)loud installed Tested Podman Client / Server 4.1.0 / 4.3.0 G OK Operator SDK v1.18.0, v1.18.1, v1.19.1 G OK Go go1.17.6 (operator-sdk v1.18.0, v1.18.1 ) (worked also with v1.19.1 ) G OK Go go1.17.8 (operator-sdk v1.19.1) G NOT TESTED Kubernetes cluster (VPC) 1.23.6_1527 C OK kubectl client 1.23 L OK kubectl server v1.23.6+IKS C OK oc server 4.7.0 C OK oc client 4.10.9 C OK operator-database/bin/ opm v1.21.0 L OK operator-application/bin/ opm v1.21.0 L OK bash GNU bash, version 3.2.57(1)-release (x86_64-apple-darwin21) L OK sed 12.3.1 L OK awk awk version 20200816 L OK cURL 7.79.1 L OK grep 2.6.0-FreeBSD L OK tar bsdtar 3.5.1 L OK container registry DockerHub , Quay(Red Hat) C OK IBM Cloud Services used Version Tested IBM Cloud Kubernetess Service (on VPC) v1.23.6+IKS OK Red Hat OpenShift on IBM Cloud (on VPC) 4.10.9_1515 OK IBM Cloud Object Storage TBD TBD IBM Cloud 'Virtual Private Cloud' TBD TBD Reference: Known problems \u00b6 Table of verifications Operator Cluster Type Region Security Group Public Gateway Container Registry Tested Note Database Kubernetes (1.23.6_1527) VPC us-south Validated Validated Quay.io OK Application (microservice) Kubernetes (1.23.6_1527) VPC us-south Validated Validated DockerHub Problems Can't be pulled from DockerHub. Solution delete repo on DockerHub and recreate it. Application OpenShift VPC us-south Validated Validated DockerHub and Quay Problems Instance of operators.coreos.com can't be deleted. Application OpenShift VPC us-south Validated Validated DockerHub and Quay Problems Operator instance can't be created when using the automation script.","title":"4.3 Parameter Reference"},{"location":"automation-version-reference/#version-reference","text":"The following information defines the environments in which the scripts have been tested, and tested versions of the prerequisite components. An explanation of how the scripts work is also provided.","title":"Version Reference"},{"location":"automation-version-reference/#tested-workstation-environments","text":"The scripts have been validated for the following technical environments: OS Version Tested macOS 12.3.1 OK macOS 11.2.3 OK","title":"Tested workstation environments"},{"location":"automation-version-reference/#tested-version-of-tools-and-frameworks-for-the-prerequsites","text":"Tools or frameworks Version/s (G)obal, (L) ocal or (C)loud installed Tested Podman Client / Server 4.1.0 / 4.3.0 G OK Operator SDK v1.18.0, v1.18.1, v1.19.1 G OK Go go1.17.6 (operator-sdk v1.18.0, v1.18.1 ) (worked also with v1.19.1 ) G OK Go go1.17.8 (operator-sdk v1.19.1) G NOT TESTED Kubernetes cluster (VPC) 1.23.6_1527 C OK kubectl client 1.23 L OK kubectl server v1.23.6+IKS C OK oc server 4.7.0 C OK oc client 4.10.9 C OK operator-database/bin/ opm v1.21.0 L OK operator-application/bin/ opm v1.21.0 L OK bash GNU bash, version 3.2.57(1)-release (x86_64-apple-darwin21) L OK sed 12.3.1 L OK awk awk version 20200816 L OK cURL 7.79.1 L OK grep 2.6.0-FreeBSD L OK tar bsdtar 3.5.1 L OK container registry DockerHub , Quay(Red Hat) C OK IBM Cloud Services used Version Tested IBM Cloud Kubernetess Service (on VPC) v1.23.6+IKS OK Red Hat OpenShift on IBM Cloud (on VPC) 4.10.9_1515 OK IBM Cloud Object Storage TBD TBD IBM Cloud 'Virtual Private Cloud' TBD TBD","title":"Tested version of tools and frameworks for the prerequsites"},{"location":"automation-version-reference/#reference-known-problems","text":"Table of verifications Operator Cluster Type Region Security Group Public Gateway Container Registry Tested Note Database Kubernetes (1.23.6_1527) VPC us-south Validated Validated Quay.io OK Application (microservice) Kubernetes (1.23.6_1527) VPC us-south Validated Validated DockerHub Problems Can't be pulled from DockerHub. Solution delete repo on DockerHub and recreate it. Application OpenShift VPC us-south Validated Validated DockerHub and Quay Problems Instance of operators.coreos.com can't be deleted. Application OpenShift VPC us-south Validated Validated DockerHub and Quay Problems Operator instance can't be created when using the automation script.","title":"Reference: Known problems"},{"location":"basic-capabilities-overview/","text":"Overview Basic Capabilities \u00b6 Creating and updating Resources Deleting Resources Storing State of Resources with Conditions Finding out the Kubernetes Versions and Capabilities Configuring Webhooks Initialization and Validation Webhooks Converting Custom Resource Versions Defining Dependencies Deploying Operators with the Operator Lifecycle Manager","title":"6.1 Overview"},{"location":"basic-capabilities-overview/#overview-basic-capabilities","text":"Creating and updating Resources Deleting Resources Storing State of Resources with Conditions Finding out the Kubernetes Versions and Capabilities Configuring Webhooks Initialization and Validation Webhooks Converting Custom Resource Versions Defining Dependencies Deploying Operators with the Operator Lifecycle Manager","title":"Overview Basic Capabilities"},{"location":"demos-auto-scaler/","text":"Auto-scaler \u00b6 Creating a Front-end Application \u00b6 The Application operator will automaticly scale the front-end application, but first we need to use the application operator to install the front-end. This is achieved by creating a resource with a \u2018kind\u2019 that correlates to the CRD already installed by the application operator. Resources (instances of a CRD) can be created manually via the operator\u2019s UI, or using a yaml file. The demo shows how to create an instance of the Application resource to trigger the operator to deploy a front end application. Note the kind is a custom resource, defined by the application operator, and the fields are specific to our application. We don\u2019t need to create lots of Kuberenetes resources ourselves, the Application resource provides an abstraction. The operator will reconcile this resource and create multiple Kubernetes resources (i.e. Deployment, Service etc) in the application-beta namespace. The Deployment creates a specified number of instances of the simple-microservice application. The simple-microservice application connects to the database-service application, and renders a Hello World response for each name in the database, when accessed by its /hello endpoint. Prometheus Metrics \u00b6 The simple-microservice application publishes metrics which are collected by Prometheus monitoring, which is installed by default on OpenShift. In particular, the metric indicates how many times the /hello endpoint has been invoked. It is on the basis of this data that auto-scaling decisions are made. Prometheus metrics can be manually queried from the OpenShift dashboard, search for application_net_heidloff_GreetingResource_countHelloEndpointInvoked_total Auto-scaling Decision Logic \u00b6 For this demo, the auto-scaling decision logic is simple. If the /hello endpoint has been invoked more than 6 times, the number of number of instances of the simple-microservice (front-end web app) should be increased. There are Kubernetes native technologies such as Horizontal Pod Autoscaler (HPA) which could achieve the same results for our simple criteria. However, this project doesn't use HPA. Instead the application operator created a CronJob which launches the operator-application-scaler application on a schedule to query the application metrics collected by Prometheus. Taking this approach brings complete flexibility to code the scaling logic. It is not bound to the metrics of just a single pod. It could consider multiples KPIs assosiated with a more complex solution, e.g. web app load, messaging queue depths, active connections to back-end etc. Commands to demo the database backup use case \u00b6 Create the Application resource: cd operator-application oc apply -f config/samples/application.sample_v1beta1_application.yaml Invoke the /hello endpoint more than 6 times, either via the OpenShift or use this command: kubectl exec -n application-beta $(kubectl get pods -n application-beta | awk '/application-deployment-microservice/ {print $1;exit}') --container application-microservice -- curl http://localhost:8081/hello Manually trigger the CronJob which launches the operator-application-scaler application: kubectl create job --from=cronjob/application-scaler manuallytriggered -n application-beta How Does it Work? \u00b6 The operator-application-scaler application is launched on a schedule by the CronJob, created by the application operator when it reconciled the Application custom resource. The operator-application-scaler is an application written in Go. It uses a Prometheus Go module to connects to Prometheus Monitoring and query the count for the /hello endpoint. If it is greater than 6, the operator-application-scaler will scale up the number of simple-microservice applications. The operator-application-scaler application does this by directly editing the Application custom resource that we created previously. There could be many Application custom resources defined in the Kubernetes cluster, each with different properties (a trivial example would be that each resource configured a unique title for the web app, which the operator duely configured via property based customisations, enabling a common web app to render the differing titles). Fortunately, when the operator created the CronJob which launches the operator-application-scaler application, it set an environment variable which specifies which Application custom resource the operator-application-scaler application should scale up or down. With this knowledge, the operator-application-scaler application uses Kubernetes API to locate the relevant Application custom resource, and change the desired state regarding the number of instances. Remember, Application custom resources are managed by the application operator. Anytime an instance of this resource is created or changed, the application operator will be notified. The operator will reconcile the new desired state by updating the Kubernetes Deployment which scales up the number of simple-microservice application pods.","title":"3.5 Auto-scaler"},{"location":"demos-auto-scaler/#auto-scaler","text":"","title":"Auto-scaler"},{"location":"demos-auto-scaler/#creating-a-front-end-application","text":"The Application operator will automaticly scale the front-end application, but first we need to use the application operator to install the front-end. This is achieved by creating a resource with a \u2018kind\u2019 that correlates to the CRD already installed by the application operator. Resources (instances of a CRD) can be created manually via the operator\u2019s UI, or using a yaml file. The demo shows how to create an instance of the Application resource to trigger the operator to deploy a front end application. Note the kind is a custom resource, defined by the application operator, and the fields are specific to our application. We don\u2019t need to create lots of Kuberenetes resources ourselves, the Application resource provides an abstraction. The operator will reconcile this resource and create multiple Kubernetes resources (i.e. Deployment, Service etc) in the application-beta namespace. The Deployment creates a specified number of instances of the simple-microservice application. The simple-microservice application connects to the database-service application, and renders a Hello World response for each name in the database, when accessed by its /hello endpoint.","title":"Creating a Front-end Application"},{"location":"demos-auto-scaler/#prometheus-metrics","text":"The simple-microservice application publishes metrics which are collected by Prometheus monitoring, which is installed by default on OpenShift. In particular, the metric indicates how many times the /hello endpoint has been invoked. It is on the basis of this data that auto-scaling decisions are made. Prometheus metrics can be manually queried from the OpenShift dashboard, search for application_net_heidloff_GreetingResource_countHelloEndpointInvoked_total","title":"Prometheus Metrics"},{"location":"demos-auto-scaler/#auto-scaling-decision-logic","text":"For this demo, the auto-scaling decision logic is simple. If the /hello endpoint has been invoked more than 6 times, the number of number of instances of the simple-microservice (front-end web app) should be increased. There are Kubernetes native technologies such as Horizontal Pod Autoscaler (HPA) which could achieve the same results for our simple criteria. However, this project doesn't use HPA. Instead the application operator created a CronJob which launches the operator-application-scaler application on a schedule to query the application metrics collected by Prometheus. Taking this approach brings complete flexibility to code the scaling logic. It is not bound to the metrics of just a single pod. It could consider multiples KPIs assosiated with a more complex solution, e.g. web app load, messaging queue depths, active connections to back-end etc.","title":"Auto-scaling Decision Logic"},{"location":"demos-auto-scaler/#commands-to-demo-the-database-backup-use-case","text":"Create the Application resource: cd operator-application oc apply -f config/samples/application.sample_v1beta1_application.yaml Invoke the /hello endpoint more than 6 times, either via the OpenShift or use this command: kubectl exec -n application-beta $(kubectl get pods -n application-beta | awk '/application-deployment-microservice/ {print $1;exit}') --container application-microservice -- curl http://localhost:8081/hello Manually trigger the CronJob which launches the operator-application-scaler application: kubectl create job --from=cronjob/application-scaler manuallytriggered -n application-beta","title":"Commands to demo the database backup use case"},{"location":"demos-auto-scaler/#how-does-it-work","text":"The operator-application-scaler application is launched on a schedule by the CronJob, created by the application operator when it reconciled the Application custom resource. The operator-application-scaler is an application written in Go. It uses a Prometheus Go module to connects to Prometheus Monitoring and query the count for the /hello endpoint. If it is greater than 6, the operator-application-scaler will scale up the number of simple-microservice applications. The operator-application-scaler application does this by directly editing the Application custom resource that we created previously. There could be many Application custom resources defined in the Kubernetes cluster, each with different properties (a trivial example would be that each resource configured a unique title for the web app, which the operator duely configured via property based customisations, enabling a common web app to render the differing titles). Fortunately, when the operator created the CronJob which launches the operator-application-scaler application, it set an environment variable which specifies which Application custom resource the operator-application-scaler application should scale up or down. With this knowledge, the operator-application-scaler application uses Kubernetes API to locate the relevant Application custom resource, and change the desired state regarding the number of instances. Remember, Application custom resources are managed by the application operator. Anytime an instance of this resource is created or changed, the application operator will be notified. The operator will reconcile the new desired state by updating the Kubernetes Deployment which scales up the number of simple-microservice application pods.","title":"How Does it Work?"},{"location":"demos-database-backup/","text":"Database Backup \u00b6 Creating a Database Cluster \u00b6 Before we can explore how the database operator automates backup, we first need to create an instance of the database. This is achieved by creating a resources with a \u2018kind\u2019 that correlates to the CRDs already installed by the database operator. Resources (instances of a CRD) can be created manually via the operator\u2019s UI, or using a yaml file. The demo shows how to create an instance of the DatabaseCluster resource to trigger the operator to deploy a database cluster. Note the kind is a custom resource, defined by the database operator, and the fields are specific to our database. We don\u2019t need to create lots of Kuberenetes resources ourselves, the DatabaseCluster resource provides an abstraction. The operator will reconcile this resource and create multiple Kubernetes resources (i.e. StatefulSet, Service etc) in the database namespace. The StatefulSet creates a specified number of instances of the database-service application. One pod is the leader supporting reads and writes, the others are followers providing only read access using replicated data. The pods use the index number of the StatefulSet to determine the leader (index 0 is leader), and each pod can communicate with the others via a Kubernetes Service to replicate data (using APIs from the database-service application). The database cluster automatically populates itself with some harcoded sample data (a simple JSON file containing a list of names). It also provides a /person endpoint to return the data: Defining a Database Backup \u00b6 We can use the databse operator to automate the day 2 task of scheduling a backup, by copying data to a backup storage repository. Instead of creating a script or runbook to be executed manually by a human operator, we can create an instance of the DatabaseBackup resource . The DatabaseBackup resource defines a list of backup storage repositories. The sample yaml defines just one backup storage repository consisting of the connection details for Cloud Object Storage on IBM Cloud. The database operator does not create the Cloud Object Storage instance. This is technically possible by using an IBM Cloud Operator , but it is not implemented. Other public clouds also have operators similar to the IBM Cloud Operator. In addition, there are two further sections to define either an immediate or scheduled backup. The database operator will reconcile the DatabaseBackup resource by creating a Kubernetes CronJob which launches the operator-database-backup application on a schedule. The DatabaseBackup resource defines the container registry & image name for the operator-database-backup application which should be defined in the CronJob, when created by the operator. Commands to demo the database backup use case \u00b6 Create the DatabaseCluster resource: cd operator-database oc apply -f config/samples/database.sample_v1alpha1_databasecluster.yaml Test the database-service from its terminal kubectl exec -n database database-cluster-0 --container database-app -- curl -s http://localhost:8089/api/persons Create the DatabaseBackup resource: cd operator-database oc apply -f config/samples/database.sample_v1alpha1_databasebackup.yaml Manually trigger the CronJob which launches the operator-database-backup application: kubectl create job --from=cronjob/databasebackup-manual-cronjob manuallytriggered -n database How Does it Work? \u00b6 The DatabaseBackup defines the connection string for the Cloud Object Storage, and references a Secret that must be present on the cluster. The operator-database-backup application is written in Go. When launched by the CronJob, it uses the Kubernetes Service to make a HTTP request to the /persons API (exposed by the database-service application) to retrieve the data. It then uses HTTP to connect to cloud object storage on IBM Cloud, creates a bucket and uploads the data. If the DatabaseBackup resource defined a scheduledTrigger section, the operator creates a CronJob which will launched the operator-database-backup application on the defined schedule. If the DatabaseBackup resource defined a manualTrigger section, the operator creates a Job which immediately launches the operator-database-backup application.","title":"3.4 Database backup"},{"location":"demos-database-backup/#database-backup","text":"","title":"Database Backup"},{"location":"demos-database-backup/#creating-a-database-cluster","text":"Before we can explore how the database operator automates backup, we first need to create an instance of the database. This is achieved by creating a resources with a \u2018kind\u2019 that correlates to the CRDs already installed by the database operator. Resources (instances of a CRD) can be created manually via the operator\u2019s UI, or using a yaml file. The demo shows how to create an instance of the DatabaseCluster resource to trigger the operator to deploy a database cluster. Note the kind is a custom resource, defined by the database operator, and the fields are specific to our database. We don\u2019t need to create lots of Kuberenetes resources ourselves, the DatabaseCluster resource provides an abstraction. The operator will reconcile this resource and create multiple Kubernetes resources (i.e. StatefulSet, Service etc) in the database namespace. The StatefulSet creates a specified number of instances of the database-service application. One pod is the leader supporting reads and writes, the others are followers providing only read access using replicated data. The pods use the index number of the StatefulSet to determine the leader (index 0 is leader), and each pod can communicate with the others via a Kubernetes Service to replicate data (using APIs from the database-service application). The database cluster automatically populates itself with some harcoded sample data (a simple JSON file containing a list of names). It also provides a /person endpoint to return the data:","title":"Creating a Database Cluster"},{"location":"demos-database-backup/#defining-a-database-backup","text":"We can use the databse operator to automate the day 2 task of scheduling a backup, by copying data to a backup storage repository. Instead of creating a script or runbook to be executed manually by a human operator, we can create an instance of the DatabaseBackup resource . The DatabaseBackup resource defines a list of backup storage repositories. The sample yaml defines just one backup storage repository consisting of the connection details for Cloud Object Storage on IBM Cloud. The database operator does not create the Cloud Object Storage instance. This is technically possible by using an IBM Cloud Operator , but it is not implemented. Other public clouds also have operators similar to the IBM Cloud Operator. In addition, there are two further sections to define either an immediate or scheduled backup. The database operator will reconcile the DatabaseBackup resource by creating a Kubernetes CronJob which launches the operator-database-backup application on a schedule. The DatabaseBackup resource defines the container registry & image name for the operator-database-backup application which should be defined in the CronJob, when created by the operator.","title":"Defining a Database Backup"},{"location":"demos-database-backup/#commands-to-demo-the-database-backup-use-case","text":"Create the DatabaseCluster resource: cd operator-database oc apply -f config/samples/database.sample_v1alpha1_databasecluster.yaml Test the database-service from its terminal kubectl exec -n database database-cluster-0 --container database-app -- curl -s http://localhost:8089/api/persons Create the DatabaseBackup resource: cd operator-database oc apply -f config/samples/database.sample_v1alpha1_databasebackup.yaml Manually trigger the CronJob which launches the operator-database-backup application: kubectl create job --from=cronjob/databasebackup-manual-cronjob manuallytriggered -n database","title":"Commands to demo the database backup use case"},{"location":"demos-database-backup/#how-does-it-work","text":"The DatabaseBackup defines the connection string for the Cloud Object Storage, and references a Secret that must be present on the cluster. The operator-database-backup application is written in Go. When launched by the CronJob, it uses the Kubernetes Service to make a HTTP request to the /persons API (exposed by the database-service application) to retrieve the data. It then uses HTTP to connect to cloud object storage on IBM Cloud, creates a bucket and uploads the data. If the DatabaseBackup resource defined a scheduledTrigger section, the operator creates a CronJob which will launched the operator-database-backup application on the defined schedule. If the DatabaseBackup resource defined a manualTrigger section, the operator creates a Job which immediately launches the operator-database-backup application.","title":"How Does it Work?"},{"location":"demos-operatorhub/","text":"Installation via OperatorHub \u00b6 Introduction to OpenShift OperatorHub \u00b6 Kubernetes includes an Operator Lifecycle Manager component which is used extensively by OpenShift to manage its own operators, and you can use it to manage your custom operator(s). In the OpenShift UI console, you\u2019ll find the OperatorHub. Each tile represents an operator which are visible in the UI of the thousands of OpenShift clusters, used by enterprises worldwide. Currently, there are around 400 operators from RedHat, IBM and other ISVs and you can filter the operators in various ways. The OperatorHub collates its contents from various sources (known as Catalogs). Red Hat Operators - Red Hat products packaged and shipped by Red Hat. Certified Operators - Products from leading independent software vendors (ISVs), and there is a simple process with RedHat to verify the operator and solution meet their quality criteria. Red Hat Marketplace \u2013 these are certified operators that are available to purchase via the Red Hat marketplace. Community Operators \u2013 these are open source operators from a specific github repo, into which it is very easy to add content. These community operators also appear in another marketplace called OperatorHub.io. Although this has a similar name to the OperatorHub in OpenShift, it\u2019s a separate marketplace aimed at any Kubernetes platform. It\u2019s also possible to create your own private catalog sources to provide your operators on a specific cluster (without deploying to public catalog). Packaging with OLM \u00b6 It\u2019s important to understand how the packaging of operators works. To have your own operator included in any of the catalog sources referenced above, it needs to be packaged according to the requirements of the Operator Lifecycle Manager, which in simplified terms looks like this: The bundle is a collection of files which includes a resource definition called a ClusterServiceVersion which describes the operator and where to find it in a registry, the manifests to create the CRDs the operator manages, and any dependencies such as other operators. From the bundle, you must create a Dockerfile to build a minimal container containing the files, which is pushed to a registry. Finally, you must create another container called the index image. The existing (or additional/new) catalog sources derive their content from index images, as they provide an API which publishes the details of your bundle. The open source operator SDK tool not only helps to scaffold operators in Go (and other languages), but also helps automate these packaging steps too. Once you have these packaging artifacts, what you do next depends on which catalog you want your operator to appear in, e.g. for Red Hat Marketplace you would work with Red Hat, for a community operator you would create a pull request and commit your bundle to the relevant open source repo, or you could simply provide the packaging artifacts directly to a customer for installation on a specific cluster. With Catalog sources installed to the cluster and the bundle, catalog and operator images uploaded to a container registry, the operators would be ready to install via the OperatorHub UI or OpenShift CLI. Custom Resource Definitions \u00b6 Each operator defines Custom Resource Definitions, AKI APIs as they are extensions to the default Kubernetes APIs for creating typical Kubernetes resources such as Deployments, Secrets etc. The database operator defines three CRDs: DatabaseCluster API/CRD allows for creating a \u2018cluster\u2019 of pods which simulate a typical database. Each pod is managed by a stateful set, has some storage, and provides some APIs. The database pods communicate with each other to establish one leader, to which data can be written and multiple followers/replicas which replicate the data for high availability. The database itself is just a sample, when called by an API, it writes a single json file to its associated persistent storage. Database API/CRD instructs the database cluster to create a \u2018database schema\u2019, populated by the specified SQL file. A real database may have other ways to create the schema. Backup API/CRD is used to automate the day 2 operation of triggering a manual or scheduled backup of the data, to cloud object storage. The operator encapsulates all the relevant know how to perform backups in a consistent and repeatable way. The application operator defines one CRD: Application API/CRD creates a frontend web application by creating Kubernetes resources like secrets and deployments, and in addition, it uses the custom resources to create a \u2018database schema\u2019 (NB. It doesn't actually do this but this was the intention for this CR. For now, the database is currently hardcoded with its data). Commands to demo the installation use case \u00b6 In this section of the demo, the operators were deployed using the OpenShift OperatorHub UI. The same result can be achieved with CLI, which may be useful if testing with Kubernetes which does not have a user interface to manage operators. For OpenShift: \u00b6 cd operator-application/olm kubectl apply -f catalogsource-openshift.yaml kubectl apply -f subscription-openshift.yaml cd operator-database/olm kubectl apply -f catalogsource-openshift.yaml kubectl apply -f subscription-openshift.yaml For Kubernetes: \u00b6 cd operator-application/olm kubectl apply -f catalogsource.yaml kubectl apply -f subscription.yaml cd operator-database/olm kubectl apply -f catalogsource-openshift.yaml kubectl apply -f subscription-openshift.yaml How Does it Work? \u00b6 The automation scripts in the Sample Operator Go project create the following resources (using the Application operator as an example): A CatalogSource - this extends the OperatorHub with two additional private catalog sources. They list the operators defined by the catalog image referenced in the Catalog A Subscription . The same resource would be created if the operator was manually installed via the OperatorHub. The subscription causes the Kubernetes OLM component to install the operator, its CRDs and any dependant operators, as defined in the bundle files (i.e. the cluster service version) exposed by the catalog image. The operator controller is deployed with a Deployment in the openshift-operators namespace. The operator does not deploy any resources until an instance of its CRD is created. This will trigger operator's reconcile loop which uses Kubernetes API to create and manage the required Kubernetes resources. The operator's CRDs can be found in Administration->Custom Resource Definitions.","title":"3.3 OperatorHub"},{"location":"demos-operatorhub/#installation-via-operatorhub","text":"","title":"Installation via OperatorHub"},{"location":"demos-operatorhub/#introduction-to-openshift-operatorhub","text":"Kubernetes includes an Operator Lifecycle Manager component which is used extensively by OpenShift to manage its own operators, and you can use it to manage your custom operator(s). In the OpenShift UI console, you\u2019ll find the OperatorHub. Each tile represents an operator which are visible in the UI of the thousands of OpenShift clusters, used by enterprises worldwide. Currently, there are around 400 operators from RedHat, IBM and other ISVs and you can filter the operators in various ways. The OperatorHub collates its contents from various sources (known as Catalogs). Red Hat Operators - Red Hat products packaged and shipped by Red Hat. Certified Operators - Products from leading independent software vendors (ISVs), and there is a simple process with RedHat to verify the operator and solution meet their quality criteria. Red Hat Marketplace \u2013 these are certified operators that are available to purchase via the Red Hat marketplace. Community Operators \u2013 these are open source operators from a specific github repo, into which it is very easy to add content. These community operators also appear in another marketplace called OperatorHub.io. Although this has a similar name to the OperatorHub in OpenShift, it\u2019s a separate marketplace aimed at any Kubernetes platform. It\u2019s also possible to create your own private catalog sources to provide your operators on a specific cluster (without deploying to public catalog).","title":"Introduction to OpenShift OperatorHub"},{"location":"demos-operatorhub/#packaging-with-olm","text":"It\u2019s important to understand how the packaging of operators works. To have your own operator included in any of the catalog sources referenced above, it needs to be packaged according to the requirements of the Operator Lifecycle Manager, which in simplified terms looks like this: The bundle is a collection of files which includes a resource definition called a ClusterServiceVersion which describes the operator and where to find it in a registry, the manifests to create the CRDs the operator manages, and any dependencies such as other operators. From the bundle, you must create a Dockerfile to build a minimal container containing the files, which is pushed to a registry. Finally, you must create another container called the index image. The existing (or additional/new) catalog sources derive their content from index images, as they provide an API which publishes the details of your bundle. The open source operator SDK tool not only helps to scaffold operators in Go (and other languages), but also helps automate these packaging steps too. Once you have these packaging artifacts, what you do next depends on which catalog you want your operator to appear in, e.g. for Red Hat Marketplace you would work with Red Hat, for a community operator you would create a pull request and commit your bundle to the relevant open source repo, or you could simply provide the packaging artifacts directly to a customer for installation on a specific cluster. With Catalog sources installed to the cluster and the bundle, catalog and operator images uploaded to a container registry, the operators would be ready to install via the OperatorHub UI or OpenShift CLI.","title":"Packaging with OLM"},{"location":"demos-operatorhub/#custom-resource-definitions","text":"Each operator defines Custom Resource Definitions, AKI APIs as they are extensions to the default Kubernetes APIs for creating typical Kubernetes resources such as Deployments, Secrets etc. The database operator defines three CRDs: DatabaseCluster API/CRD allows for creating a \u2018cluster\u2019 of pods which simulate a typical database. Each pod is managed by a stateful set, has some storage, and provides some APIs. The database pods communicate with each other to establish one leader, to which data can be written and multiple followers/replicas which replicate the data for high availability. The database itself is just a sample, when called by an API, it writes a single json file to its associated persistent storage. Database API/CRD instructs the database cluster to create a \u2018database schema\u2019, populated by the specified SQL file. A real database may have other ways to create the schema. Backup API/CRD is used to automate the day 2 operation of triggering a manual or scheduled backup of the data, to cloud object storage. The operator encapsulates all the relevant know how to perform backups in a consistent and repeatable way. The application operator defines one CRD: Application API/CRD creates a frontend web application by creating Kubernetes resources like secrets and deployments, and in addition, it uses the custom resources to create a \u2018database schema\u2019 (NB. It doesn't actually do this but this was the intention for this CR. For now, the database is currently hardcoded with its data).","title":"Custom Resource Definitions"},{"location":"demos-operatorhub/#commands-to-demo-the-installation-use-case","text":"In this section of the demo, the operators were deployed using the OpenShift OperatorHub UI. The same result can be achieved with CLI, which may be useful if testing with Kubernetes which does not have a user interface to manage operators.","title":"Commands to demo the installation use case"},{"location":"demos-operatorhub/#for-openshift","text":"cd operator-application/olm kubectl apply -f catalogsource-openshift.yaml kubectl apply -f subscription-openshift.yaml cd operator-database/olm kubectl apply -f catalogsource-openshift.yaml kubectl apply -f subscription-openshift.yaml","title":"For OpenShift:"},{"location":"demos-operatorhub/#for-kubernetes","text":"cd operator-application/olm kubectl apply -f catalogsource.yaml kubectl apply -f subscription.yaml cd operator-database/olm kubectl apply -f catalogsource-openshift.yaml kubectl apply -f subscription-openshift.yaml","title":"For Kubernetes:"},{"location":"demos-operatorhub/#how-does-it-work","text":"The automation scripts in the Sample Operator Go project create the following resources (using the Application operator as an example): A CatalogSource - this extends the OperatorHub with two additional private catalog sources. They list the operators defined by the catalog image referenced in the Catalog A Subscription . The same resource would be created if the operator was manually installed via the OperatorHub. The subscription causes the Kubernetes OLM component to install the operator, its CRDs and any dependant operators, as defined in the bundle files (i.e. the cluster service version) exposed by the catalog image. The operator controller is deployed with a Deployment in the openshift-operators namespace. The operator does not deploy any resources until an instance of its CRD is created. This will trigger operator's reconcile loop which uses Kubernetes API to create and manage the required Kubernetes resources. The operator's CRDs can be found in Administration->Custom Resource Definitions.","title":"How Does it Work?"},{"location":"demos-overview/","text":"Demos Overview \u00b6 This section summarises the key use cases presented in the 20-minute demo video . Installing operators with OperatorHub Automated backup Automated scaling If you wish to perform this demo yourself, demo setup instructions are provided, using the provided scripts to provision the environment. Alternatively, your could use the instructions to install the operators manualy using OLM . Note: the demo use cases have not been tested when the operators are running locally, or deployed without OLM. These approaches are better suited to development activities.","title":"3.1 Overview"},{"location":"demos-overview/#demos-overview","text":"This section summarises the key use cases presented in the 20-minute demo video . Installing operators with OperatorHub Automated backup Automated scaling If you wish to perform this demo yourself, demo setup instructions are provided, using the provided scripts to provision the environment. Alternatively, your could use the instructions to install the operators manualy using OLM . Note: the demo use cases have not been tested when the operators are running locally, or deployed without OLM. These approaches are better suited to development activities.","title":"Demos Overview"},{"location":"demos-setup/","text":"Demo Setup \u00b6 Although the operators have been tested with IBM Cloud Kubernetes Service, the demo video and descriptions in this section relate only to Red Hat OpenShift. If you wish to test the sample operators, then follow these mostly automated steps to setup an OpenShift cluster ready for demonstration. Clone the repo \u00b6 Provision an IBM Cloud OpenShift service \u00b6 Install the workstation prerequistes \u00b6 Install all demo components to OpenShift \u00b6 Note: Ensure you are logged on to your OpenShift cluster, before you execute following command. cd scripts sh demo-openshift-operators.sh app demo reset Create a Cloud Object Storage (COS) Bucket and Access Secrets \u00b6 Create Object Storage on IBM Cloud Login to your IBM Cloud Console Search for Object Storage in Search Bar or Click on Catalog from Upper Navigation Menu, Choose Services in Type Click on the Object Storage Once clicked a tab window will open of Cloud Ojbect Storage, IBM Cloud will be selected by default Choose your pricing plan In configure your resource section: Enter Service Name Select a Resource Group Enter Tags as requied Click on Create Create service credentials From the Left Navigation Menu Click on \"Service Credentials\". Click the \"New Credential\" blue button. Enter the name of credentials. ( Example: operator-sample-go-secret ) Keep the Role to \"writer\". Click on \"Adavanced options\". Enable to the toggle to \"Include HMAC Credentials\". Click the \"Add\" button. Expand the credential name to show the \"ACCESS KEY ID\" and \"SECRET ACCESS KEY\" written in the JSON. Store the service credentials to a Kubernetes Secret. This is required for the operator-database-backup application which backs up data to Cloud Object Storage 1 Navigate to the secret.yaml location ( operator-database-backup/kubernetes ) and open the file. operator-database-backup/kubernetes nano secret.yaml 2 Edit file secret.yaml and change the following lines respectively: HmacAccessKeyId: \"Add access_key_id from Credential JSON\" HmacSecretAccessKey: \"Add ecret_access_key from Credential JSON\" 3 Create the secret on the cluster. kubectl apply -f secret.yaml Create a Route to the /hello endpoint for the micro-service application \u00b6 Before you can create and test the route, you must have deployed the Application and DatabaseCluster resources. cd operator-database oc apply -f config/samples/database.sample_v1alpha1_databasecluster.yaml cd operator-application oc apply -f config/samples/application.sample_v1beta1_application.yaml Login to Openshift console Ensure you are in the \"Administrator\" view Ensure you are in the \"application-beta\" project Go to the \"Networking\" section in left navigation Select \"Routes\" Click the \"Create Route\" button Add following details Name the route \"hello\" Add the path \"/hello\" Select Service \"application-service-microservice\" Select Target Port 8081 -> 8081 (TCP) Click on \"Create\" Now browse to the Location Link provided in the Routes List to invoke the /hello endpoint. It sometimes takes a couple of requests before the simple-microservice application responds. The browser should render the following response: Hello World and hello Adam The Route will continue to work even if the simple-microservice application is deleted and later re-created (by the Application operator) Uninstall components prior to demo \u00b6 Having followed the above steps, you should have an OpenShift cluster with all the required components to test the sample operators. However, in the demo video, certain steps were performed manually via the OpenShift console e.g. installing the operators via OperatorHub. If you wish to reproduce the demo yourself, it is necessary to delete some resources in preparation: Uninstall both operators (but leave Catalog sources), using OpenShift Console menu Installed Operators Delete all CR instances (if present): Application, Database, Databasecluster, Databasebackup, using using OpenShift Console menu Administration->CustomResourceDefinitions->[CRD Name]->Instances Delete all CRDs: Application, Database, Databasecluster, Databasebackup, using using OpenShift Console menu Administration->CustomResourceDefinitions","title":"3.2 Setup"},{"location":"demos-setup/#demo-setup","text":"Although the operators have been tested with IBM Cloud Kubernetes Service, the demo video and descriptions in this section relate only to Red Hat OpenShift. If you wish to test the sample operators, then follow these mostly automated steps to setup an OpenShift cluster ready for demonstration.","title":"Demo Setup"},{"location":"demos-setup/#clone-the-repo","text":"","title":"Clone the repo"},{"location":"demos-setup/#provision-an-ibm-cloud-openshift-service","text":"","title":"Provision an IBM Cloud OpenShift service"},{"location":"demos-setup/#install-the-workstation-prerequistes","text":"","title":"Install the workstation prerequistes"},{"location":"demos-setup/#install-all-demo-components-to-openshift","text":"Note: Ensure you are logged on to your OpenShift cluster, before you execute following command. cd scripts sh demo-openshift-operators.sh app demo reset","title":"Install all demo components to OpenShift"},{"location":"demos-setup/#create-a-cloud-object-storage-cos-bucket-and-access-secrets","text":"Create Object Storage on IBM Cloud Login to your IBM Cloud Console Search for Object Storage in Search Bar or Click on Catalog from Upper Navigation Menu, Choose Services in Type Click on the Object Storage Once clicked a tab window will open of Cloud Ojbect Storage, IBM Cloud will be selected by default Choose your pricing plan In configure your resource section: Enter Service Name Select a Resource Group Enter Tags as requied Click on Create Create service credentials From the Left Navigation Menu Click on \"Service Credentials\". Click the \"New Credential\" blue button. Enter the name of credentials. ( Example: operator-sample-go-secret ) Keep the Role to \"writer\". Click on \"Adavanced options\". Enable to the toggle to \"Include HMAC Credentials\". Click the \"Add\" button. Expand the credential name to show the \"ACCESS KEY ID\" and \"SECRET ACCESS KEY\" written in the JSON. Store the service credentials to a Kubernetes Secret. This is required for the operator-database-backup application which backs up data to Cloud Object Storage 1 Navigate to the secret.yaml location ( operator-database-backup/kubernetes ) and open the file. operator-database-backup/kubernetes nano secret.yaml 2 Edit file secret.yaml and change the following lines respectively: HmacAccessKeyId: \"Add access_key_id from Credential JSON\" HmacSecretAccessKey: \"Add ecret_access_key from Credential JSON\" 3 Create the secret on the cluster. kubectl apply -f secret.yaml","title":"Create a Cloud Object Storage (COS) Bucket and Access Secrets"},{"location":"demos-setup/#create-a-route-to-the-hello-endpoint-for-the-micro-service-application","text":"Before you can create and test the route, you must have deployed the Application and DatabaseCluster resources. cd operator-database oc apply -f config/samples/database.sample_v1alpha1_databasecluster.yaml cd operator-application oc apply -f config/samples/application.sample_v1beta1_application.yaml Login to Openshift console Ensure you are in the \"Administrator\" view Ensure you are in the \"application-beta\" project Go to the \"Networking\" section in left navigation Select \"Routes\" Click the \"Create Route\" button Add following details Name the route \"hello\" Add the path \"/hello\" Select Service \"application-service-microservice\" Select Target Port 8081 -> 8081 (TCP) Click on \"Create\" Now browse to the Location Link provided in the Routes List to invoke the /hello endpoint. It sometimes takes a couple of requests before the simple-microservice application responds. The browser should render the following response: Hello World and hello Adam The Route will continue to work even if the simple-microservice application is deleted and later re-created (by the Application operator)","title":"Create a Route to the /hello endpoint for the micro-service application"},{"location":"demos-setup/#uninstall-components-prior-to-demo","text":"Having followed the above steps, you should have an OpenShift cluster with all the required components to test the sample operators. However, in the demo video, certain steps were performed manually via the OpenShift console e.g. installing the operators via OperatorHub. If you wish to reproduce the demo yourself, it is necessary to delete some resources in preparation: Uninstall both operators (but leave Catalog sources), using OpenShift Console menu Installed Operators Delete all CR instances (if present): Application, Database, Databasecluster, Databasebackup, using using OpenShift Console menu Administration->CustomResourceDefinitions->[CRD Name]->Instances Delete all CRDs: Application, Database, Databasecluster, Databasebackup, using using OpenShift Console menu Administration->CustomResourceDefinitions","title":"Uninstall components prior to demo"},{"location":"dev-overview/","text":"Overview \u00b6 Description \u00b6 Follow these steps to prepare your workstation and install required cluster components for developing operators. Learn the different ways to deploy and test operators. Run & debug an operataor locally . This allows you to debug your operator in Visual Studio Code. Deploy an operator to a kubernetes cluster without requiring the Operator Lifecycle Manager (OLM) packaging Package and deploy an operator using OLM . This is the approach the script automation uses. OLM packaging is required to advance to sharing an operator via a catalog or marketplace. Create a Kubernetes or OpenShift cluster on IBM Cloud to test this project.","title":"2.1 Overview"},{"location":"dev-overview/#overview","text":"","title":"Overview"},{"location":"dev-overview/#description","text":"Follow these steps to prepare your workstation and install required cluster components for developing operators. Learn the different ways to deploy and test operators. Run & debug an operataor locally . This allows you to debug your operator in Visual Studio Code. Deploy an operator to a kubernetes cluster without requiring the Operator Lifecycle Manager (OLM) packaging Package and deploy an operator using OLM . This is the approach the script automation uses. OLM packaging is required to advance to sharing an operator via a catalog or marketplace. Create a Kubernetes or OpenShift cluster on IBM Cloud to test this project.","title":"Description"},{"location":"dev-prerequisites/","text":"Prerequisites \u00b6 In order to run the samples you need following: Required CLIs \u00b6 Install the required CLIs: operator-sdk , which also provides Golang. See Operator SDK installation notes below. git kubectl or oc podman Only if IBM Cloud is used: ibmcloud Operator SDK Installation \u00b6 \ud83d\udd34 IMPORTANT: There are issues with different combinations of operator-sdk and go. This repo has been tested with operator-sdk 1.19.1 and go 1.17.6 . If you don't use this combination, binaries will be missing. Brew doesn't work either. CURRENT_USER = $( id -un ) sudo go clean -cache brew uninstall operator-sdk brew uninstall go sudo rm -rf /usr/local/Cellar/go sudo rm -rf /usr/local/go sudo rm -rf /Users/ $CURRENT_USER /go mkdir operator-sdk-install cd operator-sdk-install export ARCH = $(case $( uname -m ) in x86_64 ) echo -n amd64 ;; aarch64 ) echo -n arm64 ;; * ) echo -n $( uname -m ) ;; esac ) export OS = $( uname | awk '{print tolower($0)}' ) export OPERATOR_SDK_DL_URL = https://github.com/operator-framework/operator-sdk/releases/download/v1.19.1 curl -LO ${ OPERATOR_SDK_DL_URL } /operator-sdk_ ${ OS } _ ${ ARCH } chmod +x operator-sdk_ ${ OS } _ ${ ARCH } && sudo mv operator-sdk_ ${ OS } _ ${ ARCH } /usr/local/bin/operator-sdk curl -LO https://go.dev/dl/go1.17.6. ${ OS } - ${ ARCH } .pkg sudo installer -pkg go1.17.6. ${ OS } - ${ ARCH } .pkg -target / operator-sdk version go version Editing the code with Visual Studio Code \u00b6 The repo should be cloned with this command: git clone https://github.com/ibm/operator-sample-go.git The repo contains multiple folders containing the sample operators and applications. If you open the root folder \"operator-sample-go\", you will find that VSCode reports errors in the source code. Therefore you must open multiple VSCode windows for each folder, i.e: cd operator-sample-go/operator-application code . Alternatively, if you prefer to see all folders in a single VSCode window, configure a VSCode workspace \ud83d\udd34 IMPORTANT: When lauching VSCode for the first time, the code might display import errors as shown below. To resolve the errors, follow these steps: Right-click the code folder in VSCode and select \"Open in Integrated Terminal\" Type the following command to import the required Go packages: go mod tidy Install Go extension plugin for Visual Studio Code. The VS Code Go extension provides rich language support for the Go programming language. Create Kubernetes Cluster \u00b6 Any newer Kubernetes cluster should work. You can also use OpenShift. The Operator SDK version v1.19.1 has been tested with Kubernetes v1.23. We have tested the two operators with ... IBM Cloud Kubernetes Service 1.23.6 IBM Red Hat OpenShift on IBM Cloud 4.9.28 Log in to Kubernetes or OpenShift, for example: ibmcloud login -a cloud.ibm.com -r eu-de -g resource-group-niklas-heidloff --sso ibmcloud ks cluster config --cluster xxxxxxx kubectl get all oc login --token=sha256~xxxxx --server=https://c106-e.us-south.containers.cloud.ibm.com:32335 kubectl get all Install Required Kubernetes Components \u00b6 cert-manager OLM (Operator Lifecycle Manager) Prometheus OpenShift comes with certain components preinstalled which is why there are two scripts to install the additional components (one for OpenShift, another for Kubernetes). Kubernetes sh scripts/install-required-kubernetes-components.sh Note: Although it is possible to install the sample operators without OLM, the above script installs it anyway. It is a requirement to install cert-manager and Prometheus. OpenShift sh scripts/install-required-openshift-components.sh Image Registry \u00b6 If you want to run the samples without modifications, nothing needs to be changed. If you want to change them, replace REGISTRY and ORG with your registry account and change the version numbers in versions_local.env file. Create a version_local.env file based on the template. cat versions_local.env-template > versions_local.env Open the versions_local.env in Visual Studio Code code versions_local.env Change the values to your needs, e.g. export REGISTRY = 'quay.io' export ORG = 'tsuedbroecker' export COMMON_TAG = 'v1.0.36' Open a terminal in the project and use the versions_local.env as input for your environment variables source versions_local.env podman login $REGISTRY Setup of the required executable bin files \u00b6 The repo does not contain certain bin files which are required to build operators. The bin files (controller-gen, kustomize, opm, setup-envtest) are normally added to the operator project when initially created by the operator SDK tool. A script is provided to create a temp operator SDK project, copy the bin files to sample application and database operator projects, then delete the temp project when it has finished. sh scripts/check-binfiles-for-operator-sdk-projects.sh Note: You need to interact with the script. These are the temp values you can use for the script execution: 'Display name : myproblemfix' , Description : myproblemfix , Provider's name: myproblemfix , Any relevant URL: , Comma-separated keywords : myproblemfix Comma-separated maintainers: myproblemfix@myproblemfix.net . Verify Prerequisites \u00b6 You can run to verify your workstation prerequisites with the following script . The script informs you if the tools are installed, but you need to verify the versions in the terminal output with the verified versions . cd operator-sample-go sh scripts/check-prerequisites.sh","title":"2.2 Prereqs"},{"location":"dev-prerequisites/#prerequisites","text":"In order to run the samples you need following:","title":"Prerequisites"},{"location":"dev-prerequisites/#required-clis","text":"Install the required CLIs: operator-sdk , which also provides Golang. See Operator SDK installation notes below. git kubectl or oc podman Only if IBM Cloud is used: ibmcloud","title":"Required CLIs"},{"location":"dev-prerequisites/#operator-sdk-installation","text":"\ud83d\udd34 IMPORTANT: There are issues with different combinations of operator-sdk and go. This repo has been tested with operator-sdk 1.19.1 and go 1.17.6 . If you don't use this combination, binaries will be missing. Brew doesn't work either. CURRENT_USER = $( id -un ) sudo go clean -cache brew uninstall operator-sdk brew uninstall go sudo rm -rf /usr/local/Cellar/go sudo rm -rf /usr/local/go sudo rm -rf /Users/ $CURRENT_USER /go mkdir operator-sdk-install cd operator-sdk-install export ARCH = $(case $( uname -m ) in x86_64 ) echo -n amd64 ;; aarch64 ) echo -n arm64 ;; * ) echo -n $( uname -m ) ;; esac ) export OS = $( uname | awk '{print tolower($0)}' ) export OPERATOR_SDK_DL_URL = https://github.com/operator-framework/operator-sdk/releases/download/v1.19.1 curl -LO ${ OPERATOR_SDK_DL_URL } /operator-sdk_ ${ OS } _ ${ ARCH } chmod +x operator-sdk_ ${ OS } _ ${ ARCH } && sudo mv operator-sdk_ ${ OS } _ ${ ARCH } /usr/local/bin/operator-sdk curl -LO https://go.dev/dl/go1.17.6. ${ OS } - ${ ARCH } .pkg sudo installer -pkg go1.17.6. ${ OS } - ${ ARCH } .pkg -target / operator-sdk version go version","title":"Operator SDK Installation"},{"location":"dev-prerequisites/#editing-the-code-with-visual-studio-code","text":"The repo should be cloned with this command: git clone https://github.com/ibm/operator-sample-go.git The repo contains multiple folders containing the sample operators and applications. If you open the root folder \"operator-sample-go\", you will find that VSCode reports errors in the source code. Therefore you must open multiple VSCode windows for each folder, i.e: cd operator-sample-go/operator-application code . Alternatively, if you prefer to see all folders in a single VSCode window, configure a VSCode workspace \ud83d\udd34 IMPORTANT: When lauching VSCode for the first time, the code might display import errors as shown below. To resolve the errors, follow these steps: Right-click the code folder in VSCode and select \"Open in Integrated Terminal\" Type the following command to import the required Go packages: go mod tidy Install Go extension plugin for Visual Studio Code. The VS Code Go extension provides rich language support for the Go programming language.","title":"Editing the code with Visual Studio Code"},{"location":"dev-prerequisites/#create-kubernetes-cluster","text":"Any newer Kubernetes cluster should work. You can also use OpenShift. The Operator SDK version v1.19.1 has been tested with Kubernetes v1.23. We have tested the two operators with ... IBM Cloud Kubernetes Service 1.23.6 IBM Red Hat OpenShift on IBM Cloud 4.9.28 Log in to Kubernetes or OpenShift, for example: ibmcloud login -a cloud.ibm.com -r eu-de -g resource-group-niklas-heidloff --sso ibmcloud ks cluster config --cluster xxxxxxx kubectl get all oc login --token=sha256~xxxxx --server=https://c106-e.us-south.containers.cloud.ibm.com:32335 kubectl get all","title":"Create Kubernetes Cluster"},{"location":"dev-prerequisites/#install-required-kubernetes-components","text":"cert-manager OLM (Operator Lifecycle Manager) Prometheus OpenShift comes with certain components preinstalled which is why there are two scripts to install the additional components (one for OpenShift, another for Kubernetes). Kubernetes sh scripts/install-required-kubernetes-components.sh Note: Although it is possible to install the sample operators without OLM, the above script installs it anyway. It is a requirement to install cert-manager and Prometheus. OpenShift sh scripts/install-required-openshift-components.sh","title":"Install Required Kubernetes Components"},{"location":"dev-prerequisites/#image-registry","text":"If you want to run the samples without modifications, nothing needs to be changed. If you want to change them, replace REGISTRY and ORG with your registry account and change the version numbers in versions_local.env file. Create a version_local.env file based on the template. cat versions_local.env-template > versions_local.env Open the versions_local.env in Visual Studio Code code versions_local.env Change the values to your needs, e.g. export REGISTRY = 'quay.io' export ORG = 'tsuedbroecker' export COMMON_TAG = 'v1.0.36' Open a terminal in the project and use the versions_local.env as input for your environment variables source versions_local.env podman login $REGISTRY","title":"Image Registry"},{"location":"dev-prerequisites/#setup-of-the-required-executable-bin-files","text":"The repo does not contain certain bin files which are required to build operators. The bin files (controller-gen, kustomize, opm, setup-envtest) are normally added to the operator project when initially created by the operator SDK tool. A script is provided to create a temp operator SDK project, copy the bin files to sample application and database operator projects, then delete the temp project when it has finished. sh scripts/check-binfiles-for-operator-sdk-projects.sh Note: You need to interact with the script. These are the temp values you can use for the script execution: 'Display name : myproblemfix' , Description : myproblemfix , Provider's name: myproblemfix , Any relevant URL: , Comma-separated keywords : myproblemfix Comma-separated maintainers: myproblemfix@myproblemfix.net .","title":"Setup of the required executable bin files"},{"location":"dev-prerequisites/#verify-prerequisites","text":"You can run to verify your workstation prerequisites with the following script . The script informs you if the tools are installed, but you need to verify the versions in the terminal output with the verified versions . cd operator-sample-go sh scripts/check-prerequisites.sh","title":"Verify Prerequisites"},{"location":"dev-run-operator-locally/","text":"Database Operator - Setup and local Usage \u00b6 \ud83d\udd34 IMPORTANT: First install the prerequistes ! If you don't do it, it won't work :) Run operator locally \u00b6 From a terminal run this command: $ cd operator-database $ make install run Alternatively, to debug the operator in VSCode, press F5 (Run - Start Debugging) instead of 'make install run'. The directory 'operator-application' needs to be root in VSCode. From a second terminal run this command to create an instance of the DatabaseCluster and Database custom resources: $ kubectl apply -f config/samples/database.sample_v1alpha1_database.yaml $ kubectl apply -f config/samples/database.sample_v1alpha1_databasecluster.yaml Delete all resources \u00b6 $ kubectl delete -f config/samples/database.sample_v1alpha1_databasecluster.yaml $ kubectl delete -f config/samples/database.sample_v1alpha1_database.yaml $ make uninstall Application Operator - Setup and local Usage \u00b6 \ud83d\udd34 IMPORTANT: First install the prerequistes ! If you don't do it, it won't work :) Create database custom resource definition \u00b6 The application operator will not install until the database operator is also installed. Hence to test the application operator, you must first create the required database custom resource definition. $ cd operator-application $ kubectl create namespace database $ kubectl apply -f ../operator-database/config/crd/bases/database.sample.third.party_databases.yaml Run operator locally \u00b6 From a terminal run this command: $ cd operator-application $ make install run ENABLE_WEBHOOKS = false From another terminal run this command to create an instance of the Application custom resource: $ kubectl apply -f config/samples/application.sample_v1beta1_application.yaml Alternatively, to debug the operator in VSCode (without webhooks), press F5 (Run - Start Debugging) instead of 'make install run'. The directory 'operator-application' needs to be root in VSCode. Verify the setup \u00b6 $ kubectl get applications.application.sample.ibm.com/application -n application-beta -oyaml $ kubectl exec -n application-beta $( kubectl get pods -n application-beta | awk '/application-deployment-microservice/ {print $1;exit}' ) --container application-microservice -- curl -s http://localhost:8081/hello Delete all resources \u00b6 $ kubectl delete -f config/samples/application.sample_v1beta1_application.yaml $ make uninstall","title":"2.5 Run operators locally"},{"location":"dev-run-operator-locally/#database-operator-setup-and-local-usage","text":"\ud83d\udd34 IMPORTANT: First install the prerequistes ! If you don't do it, it won't work :)","title":"Database Operator - Setup and local Usage"},{"location":"dev-run-operator-locally/#run-operator-locally","text":"From a terminal run this command: $ cd operator-database $ make install run Alternatively, to debug the operator in VSCode, press F5 (Run - Start Debugging) instead of 'make install run'. The directory 'operator-application' needs to be root in VSCode. From a second terminal run this command to create an instance of the DatabaseCluster and Database custom resources: $ kubectl apply -f config/samples/database.sample_v1alpha1_database.yaml $ kubectl apply -f config/samples/database.sample_v1alpha1_databasecluster.yaml","title":"Run operator locally"},{"location":"dev-run-operator-locally/#delete-all-resources","text":"$ kubectl delete -f config/samples/database.sample_v1alpha1_databasecluster.yaml $ kubectl delete -f config/samples/database.sample_v1alpha1_database.yaml $ make uninstall","title":"Delete all resources"},{"location":"dev-run-operator-locally/#application-operator-setup-and-local-usage","text":"\ud83d\udd34 IMPORTANT: First install the prerequistes ! If you don't do it, it won't work :)","title":"Application Operator - Setup and local Usage"},{"location":"dev-run-operator-locally/#create-database-custom-resource-definition","text":"The application operator will not install until the database operator is also installed. Hence to test the application operator, you must first create the required database custom resource definition. $ cd operator-application $ kubectl create namespace database $ kubectl apply -f ../operator-database/config/crd/bases/database.sample.third.party_databases.yaml","title":"Create database custom resource definition"},{"location":"dev-run-operator-locally/#run-operator-locally_1","text":"From a terminal run this command: $ cd operator-application $ make install run ENABLE_WEBHOOKS = false From another terminal run this command to create an instance of the Application custom resource: $ kubectl apply -f config/samples/application.sample_v1beta1_application.yaml Alternatively, to debug the operator in VSCode (without webhooks), press F5 (Run - Start Debugging) instead of 'make install run'. The directory 'operator-application' needs to be root in VSCode.","title":"Run operator locally"},{"location":"dev-run-operator-locally/#verify-the-setup","text":"$ kubectl get applications.application.sample.ibm.com/application -n application-beta -oyaml $ kubectl exec -n application-beta $( kubectl get pods -n application-beta | awk '/application-deployment-microservice/ {print $1;exit}' ) --container application-microservice -- curl -s http://localhost:8081/hello","title":"Verify the setup"},{"location":"dev-run-operator-locally/#delete-all-resources_1","text":"$ kubectl delete -f config/samples/application.sample_v1beta1_application.yaml $ make uninstall","title":"Delete all resources"},{"location":"dev-run-operator-with-olm/","text":"Database Operator - Operator deployed with OLM \u00b6 Setup of required executable bin files \u00b6 The repo does not contain certain bin files which are required to create the OLM catalog image. The bin files (controller-gen, kustomize, opm, setup-envtest) are normally added to the operator project when initially created by the operator SDK tool. A script is provided to create a temp operator SDK project, copy the bin files to sample application and database operator projects, then delete the temp project when it has finished. sh scripts/check-binfiles-for-operator-sdk-projects.sh Note: You need to interact with the script. These are the temp values you can use for the script execution: 'Display name : myproblemfix' , Description : myproblemfix , Provider's name: myproblemfix , Any relevant URL: , Comma-separated keywords : myproblemfix Comma-separated maintainers: myproblemfix@myproblemfix.net . \ud83d\udd34 IMPORTANT: First install the prerequistes ! If you don't do it, it won't work :) Deploy catalog source and subscription \u00b6 $ cd operator-database For Kubernetes: $ kubectl apply -f olm/catalogsource.yaml $ kubectl apply -f olm/subscription.yaml For OpenShift: $ kubectl apply -f olm/catalogsource-openshift.yaml $ kubectl apply -f olm/subscription-openshift.yaml Verify the setup \u00b6 For Kubernetes: $ export NAMESPACE = operators For OpenShift: $ export NAMESPACE = openshift-operators $ kubectl get all -n $NAMESPACE $ kubectl get catalogsource operator-database-catalog -n $NAMESPACE -oyaml $ kubectl get subscriptions operator-database-v0-0-1-sub -n $NAMESPACE -oyaml $ kubectl get csv operator-database.v0.0.1 -n $NAMESPACE -oyaml $ kubectl get installplans -n $NAMESPACE $ kubectl get installplans install-xxxxx -n $NAMESPACE -oyaml $ kubectl get operators operator-database. $NAMESPACE -n $NAMESPACE -oyaml $ kubectl create ns database $ kubectl apply -f config/samples/database.sample_v1alpha1_database.yaml $ kubectl apply -f config/samples/database.sample_v1alpha1_databasecluster.yaml $ kubectl get databases/database -n database -oyaml $ kubectl get databases.database.sample.third.party/database -n database -oyaml Delete all resources \u00b6 $ kubectl delete -f config/samples/database.sample_v1alpha1_databasecluster.yaml $ kubectl delete -f config/samples/database.sample_v1alpha1_database.yaml $ kubectl delete -f olm/subscription.yaml $ kubectl delete -f olm/catalogsource.yaml $ kubectl delete -f olm/subscription-openshift.yaml $ kubectl delete -f olm/catalogsource-openshift.yaml Build and push new operator image \u00b6 Create versions_local.env and change 'REGISTRY', 'ORG' and image version. $ source ../versions_local.env $ podman build -t \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR \" . $ podman push \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR \" Build and push new bundle image \u00b6 $ source ../versions_local.env $ make bundle IMG = \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR \" $ podman build -f bundle.Dockerfile -t \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR_BUNDLE \" . $ podman push \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR_BUNDLE \" Build and push new catalog image \u00b6 $ source ../versions_local.env $ ./bin/opm index add --build-tool podman --mode semver --tag \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR_CATALOG \" --bundles \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR_BUNDLE \" $ podman push \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR_CATALOG \" Note: Define \" \\(REGISTRY/\\) ORG/$IMAGE_DATABASE_OPERATOR_CATALOG\" in olm/catalogsource.yaml and/or olm/catalogsource-openshift.yaml and invoke the commands above to update the catalog source and subscription. Alternative \u00b6 The Operator SDK provides a way to deploy the operator without a catalog. $ operator-sdk run bundle \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR_BUNDLE \" -n operators or for OpenShift: $ operator-sdk run bundle \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR_BUNDLE \" -n openshift-operators Application Operator - Operator deployed with OLM \u00b6 \ud83d\udd34 IMPORTANT: First install the prerequistes ! If you don't do it, it won't work :) Deploy database operator \u00b6 Before running the application operator, the database operator needs to be deployed since it is defined as dependency. Refer to the previous section. Deploy catalog source and subscription \u00b6 $ cd operator-application For Kubernetes: $ kubectl apply -f olm/catalogsource.yaml $ kubectl apply -f olm/subscription.yaml For OpenShift: $ kubectl apply -f olm/catalogsource-openshift.yaml $ kubectl apply -f olm/subscription-openshift.yaml Verify the setup \u00b6 For Kubernetes: $ export NAMESPACE = operators For OpenShift: $ export NAMESPACE = openshift-operators $ kubectl get all -n $NAMESPACE $ kubectl get catalogsource operator-application-catalog -n $NAMESPACE -oyaml $ kubectl get subscriptions operator-application-v0-0-1-sub -n $NAMESPACE -oyaml $ kubectl get csv operator-application.v0.0.1 -n $NAMESPACE -oyaml $ kubectl get installplans -n $NAMESPACE $ kubectl get installplans install-xxxxx -n $NAMESPACE -oyaml $ kubectl get operators operator-application. $NAMESPACE -n $NAMESPACE -oyaml $ kubectl apply -f config/samples/application.sample_v1beta1_application.yaml $ kubectl get applications.application.sample.ibm.com/application -n application-beta -oyaml $ kubectl exec -n application-beta $( kubectl get pods -n application-beta | awk '/application-deployment-microservice/ {print $1;exit}' ) --container application-microservice -- curl http://localhost:8081/hello $ kubectl logs -n $NAMESPACE $( kubectl get pods -n $NAMESPACE | awk '/operator-application-controller-manager/ {print $1;exit}' ) -c manager Delete all resources \u00b6 $ kubectl delete -f config/samples/application.sample_v1beta1_application.yaml $ kubectl delete -f olm/subscription.yaml $ kubectl delete -f olm/catalogsource.yaml $ kubectl delete -f olm/subscription-openshift.yaml $ kubectl delete -f olm/catalogsource-openshift.yaml Build and push new bundle image \u00b6 Create versions_local.env and change 'REGISTRY', 'ORG' and image version. $ source ../versions_local.env $ make bundle IMG = \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR \" $ podman build -f bundle.Dockerfile -t \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR_BUNDLE \" . $ podman push \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR_BUNDLE \" Build and push new catalog image \u00b6 $ source ../versions_local.env $ ./bin/opm index add --build-tool podman --mode semver --tag \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR_CATALOG \" --bundles \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR_BUNDLE \" $ podman push \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR_CATALOG \" Define \" \\(REGISTRY/\\) ORG/$IMAGE_APPLICATION_OPERATOR_CATALOG\" in olm/catalogsource.yaml and/or olm/catalogsource-openshift.yaml and invoke the commands above to update the catalog source and subscription. Alternative \u00b6 The Operator SDK provides a way to deploy the operator without a catalog. $ operator-sdk run bundle \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR_BUNDLE \" -n operators or for OpenShift: $ operator-sdk run bundle \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR_BUNDLE \" -n openshift-operators Prometheus Metrics \u00b6 Only needed for OpenShift: These steps allow the default Prometheus instance on OpenShift to monitor the resources deployed by the application operator. In addition, because this instance is used to monitor other k8s resources, it requires authentication and can only be accessed via https. Therefore additional secrets must be created providing a certificate and bearer token which are used by the operator-application-scaler application to access the Prometheus API. Additional RBAC permissions are also required, but these are created by the application operator. $ oc label namespace application-beta openshift.io/cluster-monitoring = \"true\" $ oc get secrets -n openshift-ingress Locate the default TLS secret with type 'kubernetes.io/tls', e.g. 'deleeuw-ocp-cluster-162e406f043e20da9b0ef0731954a894-0000' oc extract secret/<default TLS secret for your cluster> --to = /tmp -n openshift-ingress kubectl create secret generic prometheus-cert-secret --from-file = /tmp/tls.crt -n application-beta oc sa get-token -n openshift-monitoring prometheus-k8s > /tmp/token.txt kubectl create secret generic prometheus-token-secret --from-file = /tmp/token.txt -n application-beta","title":"2.7 Run operators with OLM"},{"location":"dev-run-operator-with-olm/#database-operator-operator-deployed-with-olm","text":"","title":"Database Operator - Operator deployed with OLM"},{"location":"dev-run-operator-with-olm/#setup-of-required-executable-bin-files","text":"The repo does not contain certain bin files which are required to create the OLM catalog image. The bin files (controller-gen, kustomize, opm, setup-envtest) are normally added to the operator project when initially created by the operator SDK tool. A script is provided to create a temp operator SDK project, copy the bin files to sample application and database operator projects, then delete the temp project when it has finished. sh scripts/check-binfiles-for-operator-sdk-projects.sh Note: You need to interact with the script. These are the temp values you can use for the script execution: 'Display name : myproblemfix' , Description : myproblemfix , Provider's name: myproblemfix , Any relevant URL: , Comma-separated keywords : myproblemfix Comma-separated maintainers: myproblemfix@myproblemfix.net . \ud83d\udd34 IMPORTANT: First install the prerequistes ! If you don't do it, it won't work :)","title":"Setup of required executable bin files"},{"location":"dev-run-operator-with-olm/#deploy-catalog-source-and-subscription","text":"$ cd operator-database For Kubernetes: $ kubectl apply -f olm/catalogsource.yaml $ kubectl apply -f olm/subscription.yaml For OpenShift: $ kubectl apply -f olm/catalogsource-openshift.yaml $ kubectl apply -f olm/subscription-openshift.yaml","title":"Deploy catalog source and subscription"},{"location":"dev-run-operator-with-olm/#verify-the-setup","text":"For Kubernetes: $ export NAMESPACE = operators For OpenShift: $ export NAMESPACE = openshift-operators $ kubectl get all -n $NAMESPACE $ kubectl get catalogsource operator-database-catalog -n $NAMESPACE -oyaml $ kubectl get subscriptions operator-database-v0-0-1-sub -n $NAMESPACE -oyaml $ kubectl get csv operator-database.v0.0.1 -n $NAMESPACE -oyaml $ kubectl get installplans -n $NAMESPACE $ kubectl get installplans install-xxxxx -n $NAMESPACE -oyaml $ kubectl get operators operator-database. $NAMESPACE -n $NAMESPACE -oyaml $ kubectl create ns database $ kubectl apply -f config/samples/database.sample_v1alpha1_database.yaml $ kubectl apply -f config/samples/database.sample_v1alpha1_databasecluster.yaml $ kubectl get databases/database -n database -oyaml $ kubectl get databases.database.sample.third.party/database -n database -oyaml","title":"Verify the setup"},{"location":"dev-run-operator-with-olm/#delete-all-resources","text":"$ kubectl delete -f config/samples/database.sample_v1alpha1_databasecluster.yaml $ kubectl delete -f config/samples/database.sample_v1alpha1_database.yaml $ kubectl delete -f olm/subscription.yaml $ kubectl delete -f olm/catalogsource.yaml $ kubectl delete -f olm/subscription-openshift.yaml $ kubectl delete -f olm/catalogsource-openshift.yaml","title":"Delete all resources"},{"location":"dev-run-operator-with-olm/#build-and-push-new-operator-image","text":"Create versions_local.env and change 'REGISTRY', 'ORG' and image version. $ source ../versions_local.env $ podman build -t \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR \" . $ podman push \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR \"","title":"Build and push new operator image"},{"location":"dev-run-operator-with-olm/#build-and-push-new-bundle-image","text":"$ source ../versions_local.env $ make bundle IMG = \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR \" $ podman build -f bundle.Dockerfile -t \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR_BUNDLE \" . $ podman push \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR_BUNDLE \"","title":"Build and push new bundle image"},{"location":"dev-run-operator-with-olm/#build-and-push-new-catalog-image","text":"$ source ../versions_local.env $ ./bin/opm index add --build-tool podman --mode semver --tag \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR_CATALOG \" --bundles \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR_BUNDLE \" $ podman push \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR_CATALOG \" Note: Define \" \\(REGISTRY/\\) ORG/$IMAGE_DATABASE_OPERATOR_CATALOG\" in olm/catalogsource.yaml and/or olm/catalogsource-openshift.yaml and invoke the commands above to update the catalog source and subscription.","title":"Build and push new catalog image"},{"location":"dev-run-operator-with-olm/#alternative","text":"The Operator SDK provides a way to deploy the operator without a catalog. $ operator-sdk run bundle \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR_BUNDLE \" -n operators or for OpenShift: $ operator-sdk run bundle \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR_BUNDLE \" -n openshift-operators","title":"Alternative"},{"location":"dev-run-operator-with-olm/#application-operator-operator-deployed-with-olm","text":"\ud83d\udd34 IMPORTANT: First install the prerequistes ! If you don't do it, it won't work :)","title":"Application Operator - Operator deployed with OLM"},{"location":"dev-run-operator-with-olm/#deploy-database-operator","text":"Before running the application operator, the database operator needs to be deployed since it is defined as dependency. Refer to the previous section.","title":"Deploy database operator"},{"location":"dev-run-operator-with-olm/#deploy-catalog-source-and-subscription_1","text":"$ cd operator-application For Kubernetes: $ kubectl apply -f olm/catalogsource.yaml $ kubectl apply -f olm/subscription.yaml For OpenShift: $ kubectl apply -f olm/catalogsource-openshift.yaml $ kubectl apply -f olm/subscription-openshift.yaml","title":"Deploy catalog source and subscription"},{"location":"dev-run-operator-with-olm/#verify-the-setup_1","text":"For Kubernetes: $ export NAMESPACE = operators For OpenShift: $ export NAMESPACE = openshift-operators $ kubectl get all -n $NAMESPACE $ kubectl get catalogsource operator-application-catalog -n $NAMESPACE -oyaml $ kubectl get subscriptions operator-application-v0-0-1-sub -n $NAMESPACE -oyaml $ kubectl get csv operator-application.v0.0.1 -n $NAMESPACE -oyaml $ kubectl get installplans -n $NAMESPACE $ kubectl get installplans install-xxxxx -n $NAMESPACE -oyaml $ kubectl get operators operator-application. $NAMESPACE -n $NAMESPACE -oyaml $ kubectl apply -f config/samples/application.sample_v1beta1_application.yaml $ kubectl get applications.application.sample.ibm.com/application -n application-beta -oyaml $ kubectl exec -n application-beta $( kubectl get pods -n application-beta | awk '/application-deployment-microservice/ {print $1;exit}' ) --container application-microservice -- curl http://localhost:8081/hello $ kubectl logs -n $NAMESPACE $( kubectl get pods -n $NAMESPACE | awk '/operator-application-controller-manager/ {print $1;exit}' ) -c manager","title":"Verify the setup"},{"location":"dev-run-operator-with-olm/#delete-all-resources_1","text":"$ kubectl delete -f config/samples/application.sample_v1beta1_application.yaml $ kubectl delete -f olm/subscription.yaml $ kubectl delete -f olm/catalogsource.yaml $ kubectl delete -f olm/subscription-openshift.yaml $ kubectl delete -f olm/catalogsource-openshift.yaml","title":"Delete all resources"},{"location":"dev-run-operator-with-olm/#build-and-push-new-bundle-image_1","text":"Create versions_local.env and change 'REGISTRY', 'ORG' and image version. $ source ../versions_local.env $ make bundle IMG = \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR \" $ podman build -f bundle.Dockerfile -t \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR_BUNDLE \" . $ podman push \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR_BUNDLE \"","title":"Build and push new bundle image"},{"location":"dev-run-operator-with-olm/#build-and-push-new-catalog-image_1","text":"$ source ../versions_local.env $ ./bin/opm index add --build-tool podman --mode semver --tag \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR_CATALOG \" --bundles \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR_BUNDLE \" $ podman push \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR_CATALOG \" Define \" \\(REGISTRY/\\) ORG/$IMAGE_APPLICATION_OPERATOR_CATALOG\" in olm/catalogsource.yaml and/or olm/catalogsource-openshift.yaml and invoke the commands above to update the catalog source and subscription.","title":"Build and push new catalog image"},{"location":"dev-run-operator-with-olm/#alternative_1","text":"The Operator SDK provides a way to deploy the operator without a catalog. $ operator-sdk run bundle \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR_BUNDLE \" -n operators or for OpenShift: $ operator-sdk run bundle \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR_BUNDLE \" -n openshift-operators","title":"Alternative"},{"location":"dev-run-operator-with-olm/#prometheus-metrics","text":"Only needed for OpenShift: These steps allow the default Prometheus instance on OpenShift to monitor the resources deployed by the application operator. In addition, because this instance is used to monitor other k8s resources, it requires authentication and can only be accessed via https. Therefore additional secrets must be created providing a certificate and bearer token which are used by the operator-application-scaler application to access the Prometheus API. Additional RBAC permissions are also required, but these are created by the application operator. $ oc label namespace application-beta openshift.io/cluster-monitoring = \"true\" $ oc get secrets -n openshift-ingress Locate the default TLS secret with type 'kubernetes.io/tls', e.g. 'deleeuw-ocp-cluster-162e406f043e20da9b0ef0731954a894-0000' oc extract secret/<default TLS secret for your cluster> --to = /tmp -n openshift-ingress kubectl create secret generic prometheus-cert-secret --from-file = /tmp/tls.crt -n application-beta oc sa get-token -n openshift-monitoring prometheus-k8s > /tmp/token.txt kubectl create secret generic prometheus-token-secret --from-file = /tmp/token.txt -n application-beta","title":"Prometheus Metrics"},{"location":"dev-run-operators-without-olm/","text":"Database Operator - Operator deployed without OLM \u00b6 \ud83d\udd34 IMPORTANT: First install the prerequistes ! If you don't do it, it won't work :) Deploy database operator \u00b6 $ cd operator-database $ source ../versions.env $ make deploy IMG = \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR \" $ kubectl apply -f config/rbac/role_patch.yaml $ kubectl apply -f config/rbac/role_binding_patch.yaml From a terminal run this command: $ kubectl create namespace database $ kubectl apply -f config/samples/database.sample_v1alpha1_database.yaml $ kubectl apply -f config/samples/database.sample_v1alpha1_databasecluster.yaml $ kubectl get databases/database -n database -oyaml Delete all resources \u00b6 $ kubectl delete -f config/samples/database.sample_v1alpha1_databasecluster.yaml $ kubectl delete -f config/samples/database.sample_v1alpha1_database.yaml $ make undeploy IMG = \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR \" Build and push new image \u00b6 Create versions_local.env and change 'REGISTRY', 'ORG' and image version. $ cp ../versions.env ../versions_local.env ( fill appropriate values for 'REGISTRY' & 'ORG' ) $ source ../versions_local.env $ podman build -t \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR \" . $ podman push \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR \" Application Operator - Operator deployed without OLM \u00b6 \ud83d\udd34 IMPORTANT: First install the prerequistes ! If you don't do it, it won't work :) Deploy database operator \u00b6 Before running the application operator, the database operator needs to be deployed since it is defined as dependency. See previous section. Deploy the operator \u00b6 $ cd operator-application $ source ../versions.env $ make deploy IMG = \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR \" $ kubectl apply -f config/rbac/role_patch.yaml $ kubectl apply -f config/rbac/role_binding_patch.yaml Create an instance of the Application custom resource \u00b6 $ kubectl apply -f config/samples/application.sample_v1beta1_application.yaml Verify the setup \u00b6 $ kubectl get all -n operator-application-system $ kubectl get applications.application.sample.ibm.com/application -n application-beta -oyaml $ kubectl exec -n application-beta $( kubectl get pods -n application-beta | awk '/application-deployment-microservice/ {print $1;exit}' ) --container application-microservice -- curl http://localhost:8081/hello $ kubectl logs -n operator-application-system $( kubectl get pods -n operator-application-system | awk '/operator-application-controller-manager/ {print $1;exit}' ) -c manager Delete all resources \u00b6 $ kubectl delete -f config/samples/application.sample_v1beta1_application.yaml $ make undeploy IMG = \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR \" Build and push new image \u00b6 Create versions_local.env and change 'REGISTRY', 'ORG' and image version. $ cp ../versions.env ../versions_local.env ( fill appropriate values for 'REGISTRY' & 'ORG' ) $ source ../versions_local.env $ podman build -t \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR \" . $ podman push \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR \" Prometheus Metrics \u00b6 Only needed for OpenShift: These steps allow the default Prometheus instance on OpenShift to monitor the resources deployed by the application operator. In addition, because this instance is used to monitor other k8s resources, it requires authentication and can only be accessed via https. Therefore additional secrets must be created providing a certificate and bearer token which are used by the operator-application-scaler application to access the Prometheus API. Additional RBAC permissions are also required, but these are created by the application operator. $ oc label namespace application-beta openshift.io/cluster-monitoring = \"true\" $ oc get secrets -n openshift-ingress Locate the default TLS secret with type 'kubernetes.io/tls', e.g. 'deleeuw-ocp-cluster-162e406f043e20da9b0ef0731954a894-0000' oc extract secret/<default TLS secret for your cluster> --to = /tmp -n openshift-ingress kubectl create secret generic prometheus-cert-secret --from-file = /tmp/tls.crt -n application-beta oc sa get-token -n openshift-monitoring prometheus-k8s > /tmp/token.txt kubectl create secret generic prometheus-token-secret --from-file = /tmp/token.txt -n application-beta","title":"2.6 Run operators without OLM"},{"location":"dev-run-operators-without-olm/#database-operator-operator-deployed-without-olm","text":"\ud83d\udd34 IMPORTANT: First install the prerequistes ! If you don't do it, it won't work :)","title":"Database Operator - Operator deployed without OLM"},{"location":"dev-run-operators-without-olm/#deploy-database-operator","text":"$ cd operator-database $ source ../versions.env $ make deploy IMG = \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR \" $ kubectl apply -f config/rbac/role_patch.yaml $ kubectl apply -f config/rbac/role_binding_patch.yaml From a terminal run this command: $ kubectl create namespace database $ kubectl apply -f config/samples/database.sample_v1alpha1_database.yaml $ kubectl apply -f config/samples/database.sample_v1alpha1_databasecluster.yaml $ kubectl get databases/database -n database -oyaml","title":"Deploy database operator"},{"location":"dev-run-operators-without-olm/#delete-all-resources","text":"$ kubectl delete -f config/samples/database.sample_v1alpha1_databasecluster.yaml $ kubectl delete -f config/samples/database.sample_v1alpha1_database.yaml $ make undeploy IMG = \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR \"","title":"Delete all resources"},{"location":"dev-run-operators-without-olm/#build-and-push-new-image","text":"Create versions_local.env and change 'REGISTRY', 'ORG' and image version. $ cp ../versions.env ../versions_local.env ( fill appropriate values for 'REGISTRY' & 'ORG' ) $ source ../versions_local.env $ podman build -t \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR \" . $ podman push \" $REGISTRY / $ORG / $IMAGE_DATABASE_OPERATOR \"","title":"Build and push new image"},{"location":"dev-run-operators-without-olm/#application-operator-operator-deployed-without-olm","text":"\ud83d\udd34 IMPORTANT: First install the prerequistes ! If you don't do it, it won't work :)","title":"Application Operator - Operator deployed without OLM"},{"location":"dev-run-operators-without-olm/#deploy-database-operator_1","text":"Before running the application operator, the database operator needs to be deployed since it is defined as dependency. See previous section.","title":"Deploy database operator"},{"location":"dev-run-operators-without-olm/#deploy-the-operator","text":"$ cd operator-application $ source ../versions.env $ make deploy IMG = \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR \" $ kubectl apply -f config/rbac/role_patch.yaml $ kubectl apply -f config/rbac/role_binding_patch.yaml","title":"Deploy the operator"},{"location":"dev-run-operators-without-olm/#create-an-instance-of-the-application-custom-resource","text":"$ kubectl apply -f config/samples/application.sample_v1beta1_application.yaml","title":"Create an instance of the Application custom resource"},{"location":"dev-run-operators-without-olm/#verify-the-setup","text":"$ kubectl get all -n operator-application-system $ kubectl get applications.application.sample.ibm.com/application -n application-beta -oyaml $ kubectl exec -n application-beta $( kubectl get pods -n application-beta | awk '/application-deployment-microservice/ {print $1;exit}' ) --container application-microservice -- curl http://localhost:8081/hello $ kubectl logs -n operator-application-system $( kubectl get pods -n operator-application-system | awk '/operator-application-controller-manager/ {print $1;exit}' ) -c manager","title":"Verify the setup"},{"location":"dev-run-operators-without-olm/#delete-all-resources_1","text":"$ kubectl delete -f config/samples/application.sample_v1beta1_application.yaml $ make undeploy IMG = \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR \"","title":"Delete all resources"},{"location":"dev-run-operators-without-olm/#build-and-push-new-image_1","text":"Create versions_local.env and change 'REGISTRY', 'ORG' and image version. $ cp ../versions.env ../versions_local.env ( fill appropriate values for 'REGISTRY' & 'ORG' ) $ source ../versions_local.env $ podman build -t \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR \" . $ podman push \" $REGISTRY / $ORG / $IMAGE_APPLICATION_OPERATOR \"","title":"Build and push new image"},{"location":"dev-run-operators-without-olm/#prometheus-metrics","text":"Only needed for OpenShift: These steps allow the default Prometheus instance on OpenShift to monitor the resources deployed by the application operator. In addition, because this instance is used to monitor other k8s resources, it requires authentication and can only be accessed via https. Therefore additional secrets must be created providing a certificate and bearer token which are used by the operator-application-scaler application to access the Prometheus API. Additional RBAC permissions are also required, but these are created by the application operator. $ oc label namespace application-beta openshift.io/cluster-monitoring = \"true\" $ oc get secrets -n openshift-ingress Locate the default TLS secret with type 'kubernetes.io/tls', e.g. 'deleeuw-ocp-cluster-162e406f043e20da9b0ef0731954a894-0000' oc extract secret/<default TLS secret for your cluster> --to = /tmp -n openshift-ingress kubectl create secret generic prometheus-cert-secret --from-file = /tmp/tls.crt -n application-beta oc sa get-token -n openshift-monitoring prometheus-k8s > /tmp/token.txt kubectl create secret generic prometheus-token-secret --from-file = /tmp/token.txt -n application-beta","title":"Prometheus Metrics"},{"location":"dev-setup-kubernetes/","text":"Setting up IBM Kubernetes Services \u00b6 To deploy the operators described in the repository, either a Kubernetes or an OpenShift cluster could be used. The steps represeneted in this reposiory are based on a managed Kubernetes cluster on IBM public cloud. The link provided below helps to create a Kubernetes cluster inside a VPC on IBM Cloud. In order to set up a managed Kubernetes cluster on IBM coud refer to the official documentation: Creating a cluster in your Virtual Private Cloud (VPC)","title":"2.3 Setup of IKS"},{"location":"dev-setup-kubernetes/#setting-up-ibm-kubernetes-services","text":"To deploy the operators described in the repository, either a Kubernetes or an OpenShift cluster could be used. The steps represeneted in this reposiory are based on a managed Kubernetes cluster on IBM public cloud. The link provided below helps to create a Kubernetes cluster inside a VPC on IBM Cloud. In order to set up a managed Kubernetes cluster on IBM coud refer to the official documentation: Creating a cluster in your Virtual Private Cloud (VPC)","title":"Setting up IBM Kubernetes Services"},{"location":"dev-setup-openshift/","text":"Setting up a managed OpenShift Cluster \u00b6 To deploy the operators described in the repository, either a Kubernetes or an OpenShift cluster could be used. The steps represeneted in this reposiory are based on a managed OpenShift cluster on IBM public cloud. The link provided below helps to create an OpenShift cluster inside a VPC on IBM cloud. In order to set up a managed OpenShift cluster on IBM coud refer to the official documentation: Creating an Red Hat OpenShift cluster in your Virtual Private Cloud (VPC)","title":"2.4 Setup of OpenShift"},{"location":"dev-setup-openshift/#setting-up-a-managed-openshift-cluster","text":"To deploy the operators described in the repository, either a Kubernetes or an OpenShift cluster could be used. The steps represeneted in this reposiory are based on a managed OpenShift cluster on IBM public cloud. The link provided below helps to create an OpenShift cluster inside a VPC on IBM cloud. In order to set up a managed OpenShift cluster on IBM coud refer to the official documentation: Creating an Red Hat OpenShift cluster in your Virtual Private Cloud (VPC)","title":"Setting up a managed OpenShift Cluster"},{"location":"dev-setup-vscode/","text":"Setup Visual Studio Code Environment \u00b6 Opening the root folder 'operator-sample-go' in VSCode will display import errors in the code. This can be resolved by working directly in each operator subfolder, e.g: cd operator-sample-go code . or cd operator-database make install run This method has the advantage of working only with the relevant code, albeit you will need several VSCode windows open simultaneously. Tip: you can use the following keyboard shortcut (on Mac) to switch between VSCode windows: Shift+cmd+` Alternatively, if you prefer to access all folders in a single VSCode window, you can create a VSCode workspace and add each application subfloder to it. First, open a new VSCode window. Select File->Add folder to workspace. Add each subfolder one at a time, i.e. operator-application, operator-database etc","title":"2.8 Setup Visual Studio Code Environment"},{"location":"dev-setup-vscode/#setup-visual-studio-code-environment","text":"Opening the root folder 'operator-sample-go' in VSCode will display import errors in the code. This can be resolved by working directly in each operator subfolder, e.g: cd operator-sample-go code . or cd operator-database make install run This method has the advantage of working only with the relevant code, albeit you will need several VSCode windows open simultaneously. Tip: you can use the following keyboard shortcut (on Mac) to switch between VSCode windows: Shift+cmd+` Alternatively, if you prefer to access all folders in a single VSCode window, you can create a VSCode workspace and add each application subfloder to it. First, open a new VSCode window. Select File->Add folder to workspace. Add each subfolder one at a time, i.e. operator-application, operator-database etc","title":"Setup Visual Studio Code Environment"},{"location":"golang-overview/","text":"Golang \u00b6 Importing Go Modules in Operators Accessing third Party Custom Resources in Go Operators Using object-oriented Concepts in Golang based Operators","title":"8.1 Overview"},{"location":"golang-overview/#golang","text":"Importing Go Modules in Operators Accessing third Party Custom Resources in Go Operators Using object-oriented Concepts in Golang based Operators","title":"Golang"},{"location":"intro-architecture-overview/","text":"Architecture Overview \u00b6 The image summarises the main components of the Operator Sample Go project. Note the namespace where the operators are deployed does vary: For Kubernetes the namespace is: operators For OpenShift the namespace is: openshift-operators If deployed without OLM the namespace is: operator- -system","title":"1.2 Architecture overview"},{"location":"intro-architecture-overview/#architecture-overview","text":"The image summarises the main components of the Operator Sample Go project. Note the namespace where the operators are deployed does vary: For Kubernetes the namespace is: operators For OpenShift the namespace is: openshift-operators If deployed without OLM the namespace is: operator- -system","title":"Architecture Overview"},{"location":"intro-demo-use-cases/","text":"Use Cases Overview \u00b6 Demo Video \u00b6 Our video contains a 20-minute demo section showing how a Kubernetes administrator would consume and utilise our sample operators for various use cases. You can also view a short 1-minute version of the demo . The demo videos highlight the following use cases: Introduction to OpenShift OperatorHub and packaging with OLM Installing Operators from OperatorHub Database Backup Application Auto-scaling In the demos section you can read a more detailed explanation of each demo use case, and find automated steps to to setup a demo environment.","title":"1.3 Demo use cases overview"},{"location":"intro-demo-use-cases/#use-cases-overview","text":"","title":"Use Cases Overview"},{"location":"intro-demo-use-cases/#demo-video","text":"Our video contains a 20-minute demo section showing how a Kubernetes administrator would consume and utilise our sample operators for various use cases. You can also view a short 1-minute version of the demo . The demo videos highlight the following use cases: Introduction to OpenShift OperatorHub and packaging with OLM Installing Operators from OperatorHub Database Backup Application Auto-scaling In the demos section you can read a more detailed explanation of each demo use case, and find automated steps to to setup a demo environment.","title":"Demo Video"},{"location":"intro-masterclass/","text":"Masterclass Overview \u00b6 IBM Build Labs masterclass \u00b6 This project has been created by IBM Build Labs. We provide a service to selected IBM Business Partners (ISVs) looking to automate the operations of their solutions to meet the demands of clients, and open new routes to market using Red Hat OpenShift on any cloud/platform. Our 40-minute masterclass video provides a useful inroduction to operators and why to use them: Benefits (2m14s) Automation of day 2 operations Reusability of software for multi-clouds Kubernetes-native and ecosystem Demos (12m45s) Day 1: OpenShift OperatorHub Day 2: Auto backup Day 2: Auto scalability Operators Architecture and Marketplaces (30m58s) Architecture Marketplaces Working With IBM Build Labs (38m49s) What IBM Build Labs offers How to engage with us","title":"1.4 Masterclass overview"},{"location":"intro-masterclass/#masterclass-overview","text":"","title":"Masterclass Overview"},{"location":"intro-masterclass/#ibm-build-labs-masterclass","text":"This project has been created by IBM Build Labs. We provide a service to selected IBM Business Partners (ISVs) looking to automate the operations of their solutions to meet the demands of clients, and open new routes to market using Red Hat OpenShift on any cloud/platform. Our 40-minute masterclass video provides a useful inroduction to operators and why to use them: Benefits (2m14s) Automation of day 2 operations Reusability of software for multi-clouds Kubernetes-native and ecosystem Demos (12m45s) Day 1: OpenShift OperatorHub Day 2: Auto backup Day 2: Auto scalability Operators Architecture and Marketplaces (30m58s) Architecture Marketplaces Working With IBM Build Labs (38m49s) What IBM Build Labs offers How to engage with us","title":"IBM Build Labs masterclass"},{"location":"intro-project-overview/","text":"Project Overview \u00b6 The Operator Sample Go project contains two operators and four applications: Application Operator \u00b6 The application operator deploys and manages a front-end micro-service which provides a simple web UI. The micro-service application provides a single HTTP enpoint: /hello - Renders a Hello greeting to the names persisted in the corresponding database. The application operator is responsible for the following tasks: Deployment of Kubernetes resources: \u00b6 Deployment to manage front-end Pods, Service, Roles/Role Bindings, Cronjob (to launch auto-scaling decision logic) Database (a Custom Resource reconciled by the database operator, which creates the schema) Auto-pilot operational tasks: \u00b6 Auto-scaling of pods according to how many times the /hello endpoint has been invoked The application operator can be found in the operator-application folder. Database Operator \u00b6 The database operator relates to the backend database which provides a very simple file based database, included in this project. It was created purely to help demonstrate the capabilities of operators. The database implements a cluster where one pod is the 'leader' to which applications can perform reads and writes. Further pods are 'followers' providing high availability and scalability. Followers synchronize the data from the leader, but only provide read capability. Each pod of the database cluster stores its data to a persistent volume. The database is very opinionated and only supports a single use case - storing a list of first & last names, persisted in a JSON file. Therefore only a single API is provided: /persons - GET/POST/PUT/DELETE a Person JSON object (other APIs help the pods elect the leader, and synchronize the data) The database operator is responsible for the following tasks: Deployment of Kubernetes resources: \u00b6 StatefulSet to manage stateful Pods and their binding to Persistent Volume Claims, Roles/Role Bindings, Service, Cronjob (to launch auto-backup logic) Auto-pilot operational tasks: \u00b6 Immediate or scheduled backup of the database to one or more long term repositories (e.g. Cloud Object Storage) The database operator can be found in the operator-database folder. Applications \u00b6 The repo provides the following applications, used by the operators: simple-microservice - The front end web application, written in Java using Quarkus database-service - The simple database application deployed by the database operator, written in Java using Quarkus operator-database-backup - A Go application to query the database and upload the data to cloud object storage. This container is launched on a schedule by the database operator operator-application-scaler - A Go application used to make autoscaling decisions for the front-end. It queries Prometheus metrics exposed by the simple-microservice, and if necessary, modifies the custom resource which defines the size of the front-end deployment. This container is launched on a schedule by the application operator Scripts \u00b6 The repo also includes scripts: scripts - Automation to verify workstation prerequisites, build all container images and deploy to a Kubernetes or OpenShift cluster. Alternatively, the scripts can deploy pre-built 'golden' container images. Next steps to use this project \u00b6 Get hands-on with the sample operators: Setup your workstation with the prerequisites for development Understand the basic architecture of the operators Watch the demo video Understand in detail the backup and scaling capabilities Use the script automation to build all images and deploy to IBM Cloud Kubernetes Service or IBM Cloud OpenShift Service When you're ready to experiment with changes to the sample operators: Learn how to run & debug an operataor locally Learn how to deploy an operator to a kubernetes cluster without requiring the Operator Lifecycle Manager (OLM) packaging Learn how to manually package and deploy an operator using OLM . This is the approach our script automation uses. When you're ready to create your own operators, read our companion blogs: Links to operator development resources Basic operator capabilities Advanced operator capabilities Go hints & tips related to operator development","title":"1.1 Overview"},{"location":"intro-project-overview/#project-overview","text":"The Operator Sample Go project contains two operators and four applications:","title":"Project Overview"},{"location":"intro-project-overview/#application-operator","text":"The application operator deploys and manages a front-end micro-service which provides a simple web UI. The micro-service application provides a single HTTP enpoint: /hello - Renders a Hello greeting to the names persisted in the corresponding database. The application operator is responsible for the following tasks:","title":"Application Operator"},{"location":"intro-project-overview/#deployment-of-kubernetes-resources","text":"Deployment to manage front-end Pods, Service, Roles/Role Bindings, Cronjob (to launch auto-scaling decision logic) Database (a Custom Resource reconciled by the database operator, which creates the schema)","title":"Deployment of Kubernetes resources:"},{"location":"intro-project-overview/#auto-pilot-operational-tasks","text":"Auto-scaling of pods according to how many times the /hello endpoint has been invoked The application operator can be found in the operator-application folder.","title":"Auto-pilot operational tasks:"},{"location":"intro-project-overview/#database-operator","text":"The database operator relates to the backend database which provides a very simple file based database, included in this project. It was created purely to help demonstrate the capabilities of operators. The database implements a cluster where one pod is the 'leader' to which applications can perform reads and writes. Further pods are 'followers' providing high availability and scalability. Followers synchronize the data from the leader, but only provide read capability. Each pod of the database cluster stores its data to a persistent volume. The database is very opinionated and only supports a single use case - storing a list of first & last names, persisted in a JSON file. Therefore only a single API is provided: /persons - GET/POST/PUT/DELETE a Person JSON object (other APIs help the pods elect the leader, and synchronize the data) The database operator is responsible for the following tasks:","title":"Database Operator"},{"location":"intro-project-overview/#deployment-of-kubernetes-resources_1","text":"StatefulSet to manage stateful Pods and their binding to Persistent Volume Claims, Roles/Role Bindings, Service, Cronjob (to launch auto-backup logic)","title":"Deployment of Kubernetes resources:"},{"location":"intro-project-overview/#auto-pilot-operational-tasks_1","text":"Immediate or scheduled backup of the database to one or more long term repositories (e.g. Cloud Object Storage) The database operator can be found in the operator-database folder.","title":"Auto-pilot operational tasks:"},{"location":"intro-project-overview/#applications","text":"The repo provides the following applications, used by the operators: simple-microservice - The front end web application, written in Java using Quarkus database-service - The simple database application deployed by the database operator, written in Java using Quarkus operator-database-backup - A Go application to query the database and upload the data to cloud object storage. This container is launched on a schedule by the database operator operator-application-scaler - A Go application used to make autoscaling decisions for the front-end. It queries Prometheus metrics exposed by the simple-microservice, and if necessary, modifies the custom resource which defines the size of the front-end deployment. This container is launched on a schedule by the application operator","title":"Applications"},{"location":"intro-project-overview/#scripts","text":"The repo also includes scripts: scripts - Automation to verify workstation prerequisites, build all container images and deploy to a Kubernetes or OpenShift cluster. Alternatively, the scripts can deploy pre-built 'golden' container images.","title":"Scripts"},{"location":"intro-project-overview/#next-steps-to-use-this-project","text":"Get hands-on with the sample operators: Setup your workstation with the prerequisites for development Understand the basic architecture of the operators Watch the demo video Understand in detail the backup and scaling capabilities Use the script automation to build all images and deploy to IBM Cloud Kubernetes Service or IBM Cloud OpenShift Service When you're ready to experiment with changes to the sample operators: Learn how to run & debug an operataor locally Learn how to deploy an operator to a kubernetes cluster without requiring the Operator Lifecycle Manager (OLM) packaging Learn how to manually package and deploy an operator using OLM . This is the approach our script automation uses. When you're ready to create your own operators, read our companion blogs: Links to operator development resources Basic operator capabilities Advanced operator capabilities Go hints & tips related to operator development","title":"Next steps to use this project"},{"location":"overview-and-scenarios/","text":"Operators Overview and Scenarios \u00b6 We're all familiar publishers using app stores to reach mass audiences for mobile apps. How is this possible for enterprise applications? The answer is via marketplaces. Operators support the packaging of kubernetes based enterprise applications so they can be shared and promoted in various marketplaces and catalogs. This enables enterprise applications to made available digitally in a self-service fashion. Enterprise applications are usually complex. They are often not easy to deploy or operate and may require potentially error human tasks during installation, maintenance or monitoring. Operators should be an integral part of the development of enterprise applications so they can be used to automate these complex tasks. This section describes details of: The reasons to build Kubernetes operators Technical implementation details of the day 2 capabilities provided by the sample operators Automated backup Automated scaling An introduction to the components of operator development","title":"5.1 Overview"},{"location":"overview-and-scenarios/#operators-overview-and-scenarios","text":"We're all familiar publishers using app stores to reach mass audiences for mobile apps. How is this possible for enterprise applications? The answer is via marketplaces. Operators support the packaging of kubernetes based enterprise applications so they can be shared and promoted in various marketplaces and catalogs. This enables enterprise applications to made available digitally in a self-service fashion. Enterprise applications are usually complex. They are often not easy to deploy or operate and may require potentially error human tasks during installation, maintenance or monitoring. Operators should be an integral part of the development of enterprise applications so they can be used to automate these complex tasks. This section describes details of: The reasons to build Kubernetes operators Technical implementation details of the day 2 capabilities provided by the sample operators Automated backup Automated scaling An introduction to the components of operator development","title":"Operators Overview and Scenarios"},{"location":"overview-automatically-archive-data-with-k8s-operators/","text":"Automatically Archiving Data with Kubernetes Operators \u00b6 The database operator defines a \u2018DatabaseBackup\u2018 custom resource which allows the human operator to specify when to do backups and where to store the data. apiVersion: database.sample.third.party/v1alpha1 kind: DatabaseBackup metadata: name: databasebackup-manual namespace: database spec: repos: - name: ibmcos-repo type: ibmcos secretName: ibmcos-repo serviceEndpoint: \"https://s3.fra.eu.cloud-object-storage.appdomain.cloud\" authEndpoint: \"https://iam.cloud.ibm.com/identity/token\" bucketNamePrefix: \"database-backup-\" manualTrigger: enabled: true time: \"2022-12-15T02:59:43.1Z\" repo: ibmcos-repo scheduledTrigger: enabled: false schedule: \"0 * * * *\" repo: ibmcos-repo The DatabaseBackup resource defines a list of backup storage repositories. The spec section allows for a list of backup storage reposities to be defined, but the sample yaml above defines just one backup repo with the connection details for Cloud Object Storage on IBM Cloud. There are also sections to request either an immediate one-off backup, or an repeating scheduled backup. In order to run the backups on a scheduled basis, we use Kubernetes CronJobs rather than execute the task directly from the operator's reconcile loop. This makes sense because the backup tasks can take quite some time for large datasets. The CronJob could be applied manually, but a cleaner design is to let operators do this. If the human operator creates the DatabaseBackup resource, the database operator will define a CrobJob, similar to that shown below. The properties of the CronJob are set dynamically, based on the values provided in the DatabaseBackup resource. apiVersion: batch/v1 kind: CronJob metadata: name: database-backup namespace: database spec: schedule: \"0 * * * *\" jobTemplate: spec: template: spec: containers: - name: database-backup image: docker.io/nheidloff/operator-database-backup:v1.0.117 imagePullPolicy: IfNotPresent env: - name: BACKUP_RESOURCE_NAME value: \"databasebackup-manual\" - name: NAMESPACE value: \"database\" - name: CLOUD_OBJECT_STORAGE_HMAC_ACCESS_KEY_ID value: \"xxx\" - name: CLOUD_OBJECT_STORAGE_HMAC_SECRET_ACCESS_KEY value: \"xxx\" - name: CLOUD_OBJECT_STORAGE_REGION value: \"eu-geo\" - name: CLOUD_OBJECT_STORAGE_SERVICE_ENDPOINT value: \"s3.eu.cloud-object-storage.appdomain.cloud\" restartPolicy: OnFailure If the DatabaseBackup resource defined a scheduledTrigger section, the operator creates a CronJob which will launched the operator-database-backup application on the defined schedule. If the DatabaseBackup resource defined a manualTrigger section, the operator creates a Job which immediately launches the operator-database-backup application. When the CronJob launches a job, the status.conditions of the DatabaseBackup resource provides feedback to determine whether the backup was successful. kubectl get databasebackups databasebackup-manual -n database -oyaml Example output: ... status: conditions: - lastTransitionTime: \"2022-04-07T05:16:30Z\" message: Database has been archived reason: BackupSucceeded status: \"True\" type: Succeeded kubectl logs -n database $( kubectl get pods -n database | awk '/manuallytriggered/ {print $1;exit}' ) These screenshots show the deployed CronJob, Job and resulting bucket on cloud object storage. The operator-database-backup application, launched by the CronJob is implemented in Go with the following functionality: Get the database backup resource from Kubernetes Validate input environment variables Read data from the database system Write data to object storage Write status as conditions in database backup resource","title":"5.3 Day 2 Scenario - Automatically Archiving Data"},{"location":"overview-automatically-archive-data-with-k8s-operators/#automatically-archiving-data-with-kubernetes-operators","text":"The database operator defines a \u2018DatabaseBackup\u2018 custom resource which allows the human operator to specify when to do backups and where to store the data. apiVersion: database.sample.third.party/v1alpha1 kind: DatabaseBackup metadata: name: databasebackup-manual namespace: database spec: repos: - name: ibmcos-repo type: ibmcos secretName: ibmcos-repo serviceEndpoint: \"https://s3.fra.eu.cloud-object-storage.appdomain.cloud\" authEndpoint: \"https://iam.cloud.ibm.com/identity/token\" bucketNamePrefix: \"database-backup-\" manualTrigger: enabled: true time: \"2022-12-15T02:59:43.1Z\" repo: ibmcos-repo scheduledTrigger: enabled: false schedule: \"0 * * * *\" repo: ibmcos-repo The DatabaseBackup resource defines a list of backup storage repositories. The spec section allows for a list of backup storage reposities to be defined, but the sample yaml above defines just one backup repo with the connection details for Cloud Object Storage on IBM Cloud. There are also sections to request either an immediate one-off backup, or an repeating scheduled backup. In order to run the backups on a scheduled basis, we use Kubernetes CronJobs rather than execute the task directly from the operator's reconcile loop. This makes sense because the backup tasks can take quite some time for large datasets. The CronJob could be applied manually, but a cleaner design is to let operators do this. If the human operator creates the DatabaseBackup resource, the database operator will define a CrobJob, similar to that shown below. The properties of the CronJob are set dynamically, based on the values provided in the DatabaseBackup resource. apiVersion: batch/v1 kind: CronJob metadata: name: database-backup namespace: database spec: schedule: \"0 * * * *\" jobTemplate: spec: template: spec: containers: - name: database-backup image: docker.io/nheidloff/operator-database-backup:v1.0.117 imagePullPolicy: IfNotPresent env: - name: BACKUP_RESOURCE_NAME value: \"databasebackup-manual\" - name: NAMESPACE value: \"database\" - name: CLOUD_OBJECT_STORAGE_HMAC_ACCESS_KEY_ID value: \"xxx\" - name: CLOUD_OBJECT_STORAGE_HMAC_SECRET_ACCESS_KEY value: \"xxx\" - name: CLOUD_OBJECT_STORAGE_REGION value: \"eu-geo\" - name: CLOUD_OBJECT_STORAGE_SERVICE_ENDPOINT value: \"s3.eu.cloud-object-storage.appdomain.cloud\" restartPolicy: OnFailure If the DatabaseBackup resource defined a scheduledTrigger section, the operator creates a CronJob which will launched the operator-database-backup application on the defined schedule. If the DatabaseBackup resource defined a manualTrigger section, the operator creates a Job which immediately launches the operator-database-backup application. When the CronJob launches a job, the status.conditions of the DatabaseBackup resource provides feedback to determine whether the backup was successful. kubectl get databasebackups databasebackup-manual -n database -oyaml Example output: ... status: conditions: - lastTransitionTime: \"2022-04-07T05:16:30Z\" message: Database has been archived reason: BackupSucceeded status: \"True\" type: Succeeded kubectl logs -n database $( kubectl get pods -n database | awk '/manuallytriggered/ {print $1;exit}' ) These screenshots show the deployed CronJob, Job and resulting bucket on cloud object storage. The operator-database-backup application, launched by the CronJob is implemented in Go with the following functionality: Get the database backup resource from Kubernetes Validate input environment variables Read data from the database system Write data to object storage Write status as conditions in database backup resource","title":"Automatically Archiving Data with Kubernetes Operators"},{"location":"overview-scalling-applications-automatically-with-operators/","text":"Scaling Applications automatically with Operators \u00b6 The application operator can perform automatic scaling of the front-end simple-microservice application. The simple-microservice application publishes metrics which are collected by Prometheus monitoring. Prometheus stores metrics from various sources and provides query capabilities. It is on the basis of this Prometheus data that auto-scaling decisions are made. The simple-microservice application has been implemented with Quarkus. It uses Eclipse MicroProfile to track the number of invocations of its /hello endpoint (see code ). import org.eclipse.microprofile.config.inject.ConfigProperty ; import org.eclipse.microprofile.metrics.annotation.Counted ; @Path ( \"/hello\" ) public class GreetingResource { @ConfigProperty ( name = \"greeting.message\" ) String message ; @GET @Produces ( MediaType.TEXT_PLAIN ) @Counted ( name = \"countHelloEndpointInvoked\" , description = \"How often /hello has been invoked\" ) public String hello () { return String.format ( \"Hello %s\" , message ) ; } } To allow Prometheus to scrape these metrics, a ServiceMonitor resource is used. This resource is created by the application operator (see code ) apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: app: myapplication name: myapplication-metrics-monitor namespace: application-beta spec: endpoints: - path: /q/metrics selector: matchLabels: app: myapplication The Prometheus user interface can also be used to manually query this data. To implement the scaling decision logic, a separate image/container is used. This container can be considered an extension to the application operator. The application operator sets up a CronJob for the operator-application-scaler so that it is run on a scheduled basis. The CronJob that is created by the controller looks like this. Note that the application name and namespace are passed in as parameter. apiVersion: batch/v1 kind: CronJob metadata: name: application-scaler namespace: operator-application-system spec: schedule: \"0 * * * *\" jobTemplate: spec: template: spec: containers: - name: application-scale image: docker.io/nheidloff/operator-application-scaler:v1.0.117 imagePullPolicy: IfNotPresent env: - name: APPLICATION_RESOURCE_NAME value: \"application\" - name: APPLICATION_RESOURCE_NAMESPACE value: \"application-beta\" restartPolicy: OnFailure The implementation of the actual operator-application-scaler application is trivial. It uses the Prometheus Go client library . Note that this library is still considered experimental. Alternatively consider the Prometheus HTTP API . prometheusAddress : = \"http://prometheus-operated.monitoring:9090\" queryAmountHelloEndpointInvocations : = \"application_net_heidloff_GreetingResource_countHelloEndpointInvoked_total\" client, err : = api.NewClient ( api.Config { Address: prometheusAddress, }) if err ! = nil { os.Exit ( 1 ) } v1api : = v1.NewAPI ( client ) ctx, cancel : = context.WithTimeout ( context.Background () , 10 *time.Second ) defer cancel () result, warnings, err : = v1api.Query ( ctx, queryAmountHelloEndpointInvocations, time.Now ()) if err ! = nil { os.Exit ( 1 ) } resultVector, conversionSuccessful : = ( result ) . ( model.Vector ) if conversionSuccessful == true { if resultVector.Len () > 0 { firstElement : = resultVector [ 0 ] if firstElement.Value > 5 { // Note: '5' is only used for demo purposes scaleUp () } } }","title":"5.4 Day 2 Scenario - Automatically Scaling Applications"},{"location":"overview-scalling-applications-automatically-with-operators/#scaling-applications-automatically-with-operators","text":"The application operator can perform automatic scaling of the front-end simple-microservice application. The simple-microservice application publishes metrics which are collected by Prometheus monitoring. Prometheus stores metrics from various sources and provides query capabilities. It is on the basis of this Prometheus data that auto-scaling decisions are made. The simple-microservice application has been implemented with Quarkus. It uses Eclipse MicroProfile to track the number of invocations of its /hello endpoint (see code ). import org.eclipse.microprofile.config.inject.ConfigProperty ; import org.eclipse.microprofile.metrics.annotation.Counted ; @Path ( \"/hello\" ) public class GreetingResource { @ConfigProperty ( name = \"greeting.message\" ) String message ; @GET @Produces ( MediaType.TEXT_PLAIN ) @Counted ( name = \"countHelloEndpointInvoked\" , description = \"How often /hello has been invoked\" ) public String hello () { return String.format ( \"Hello %s\" , message ) ; } } To allow Prometheus to scrape these metrics, a ServiceMonitor resource is used. This resource is created by the application operator (see code ) apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: labels: app: myapplication name: myapplication-metrics-monitor namespace: application-beta spec: endpoints: - path: /q/metrics selector: matchLabels: app: myapplication The Prometheus user interface can also be used to manually query this data. To implement the scaling decision logic, a separate image/container is used. This container can be considered an extension to the application operator. The application operator sets up a CronJob for the operator-application-scaler so that it is run on a scheduled basis. The CronJob that is created by the controller looks like this. Note that the application name and namespace are passed in as parameter. apiVersion: batch/v1 kind: CronJob metadata: name: application-scaler namespace: operator-application-system spec: schedule: \"0 * * * *\" jobTemplate: spec: template: spec: containers: - name: application-scale image: docker.io/nheidloff/operator-application-scaler:v1.0.117 imagePullPolicy: IfNotPresent env: - name: APPLICATION_RESOURCE_NAME value: \"application\" - name: APPLICATION_RESOURCE_NAMESPACE value: \"application-beta\" restartPolicy: OnFailure The implementation of the actual operator-application-scaler application is trivial. It uses the Prometheus Go client library . Note that this library is still considered experimental. Alternatively consider the Prometheus HTTP API . prometheusAddress : = \"http://prometheus-operated.monitoring:9090\" queryAmountHelloEndpointInvocations : = \"application_net_heidloff_GreetingResource_countHelloEndpointInvoked_total\" client, err : = api.NewClient ( api.Config { Address: prometheusAddress, }) if err ! = nil { os.Exit ( 1 ) } v1api : = v1.NewAPI ( client ) ctx, cancel : = context.WithTimeout ( context.Background () , 10 *time.Second ) defer cancel () result, warnings, err : = v1api.Query ( ctx, queryAmountHelloEndpointInvocations, time.Now ()) if err ! = nil { os.Exit ( 1 ) } resultVector, conversionSuccessful : = ( result ) . ( model.Vector ) if conversionSuccessful == true { if resultVector.Len () > 0 { firstElement : = resultVector [ 0 ] if firstElement.Value > 5 { // Note: '5' is only used for demo purposes scaleUp () } } }","title":"Scaling Applications automatically with Operators"},{"location":"overview-the-k8s-operator-metamodel/","text":"The Kubernetes Operator Metamodel \u00b6 This sections describes the key objects and concepts you need to understand before building operators. The diagram shows the key concepts and high level architecture, based on the Kubebuilder Architecture Concept Diagram Most of the concepts in the diagram are generic for all types of operators, no matter how they have been implemented. Some parts are specific to operators built with Golang, Operator SDK including Kubebuilder Architecture Concept Diagram and the Operator Lifecycle Manager Framework . Most of the concepts in the diagram are generic for all types of operators, no matter how they have been implemented. Some parts are specific to operators built with Golang , Operator SDK including Kubebuilder and the Operator Lifecycle Manager Framework . Below are some more details about the different objects. I have also added links to samples. The samples are part of a bigger sample, called operator-sample-go , which is available on GitHub that describes various operator patterns and best practises. Operator \u00b6 The term \u2018Operator\u2019 or \u2018Kubernetes Operator\u2019 describes the mechanism to automate deployments and day 2 operations for software running on Kubernetes which implements the operator pattern . This pattern is used by Kubernetes internally as well as externally for custom resources. Operators contain custom resource definitions and business logic to manage these resources. The self contained operators are deployed as containers on Kubernetes. Usually there is one running operator instance per cluster. For production deployments the Operator Lifecycle Manager (OLM) provides functionality to deploy and operate the operators, for example to handle multiple versions. Operators are packaged in CSVs (cluster service versions). Samples: Operator Dockerfile Operator CSV Operator initialization in main.go API \u00b6 The term API is often used as synonym to custom resource definition. Custom resource definitions have schemas and potentially multiple versions. This allows managing resources declaratively in Kubernetes production environments. Custom resource definitions are identified by their group, version and resource name. One operator can contain multiple resource definitions. Samples: Sample resource Schema as Go struct Schema as yaml Versions Manager \u00b6 The manager contains the business logic of the operator which knows how to deploy and manage custom resources. Additionally it comes with generic built in functionality to handle HA leader election, export metrics, handle webhook certs and broadcasts events. It also provides a client to access Kubernetes and a cache to improve efficiency. Sample: Manager creation in main.go Controller \u00b6 The main responsibility of controllers is to synchronize the \u2018to be\u2019 states as defined in custom resources with the \u2018as is\u2019 states in Kubernetes clusters. This includes creations of new resources, updates to existing resources or deletions. This logic is implemented in the controllers\u2019 reconcile function. The reconciler doesn\u2019t use an imperative model to manage resources because of the nature of distributed Kubernetes systems and because of the long time it can take to change resources without blocking anything. Instead the reconciler is invoked over and over again until it signals that it\u2019s done. This is why reconcilers need to be idempotent. One controller manages one custom resource definition including all versions of it. The controller uses caches and Kubernetes clients and gets events via filters. Samples: Flow in Reconcile function Synchronization of resources Creations and updates of resources Definition of resources to watch Webhooks \u00b6 With webhooks values of resources can be changed and conversions between different versions can be done. Samples: Initialization Validation Conversion","title":"5.5 The Kubernetes Operator Metamodel"},{"location":"overview-the-k8s-operator-metamodel/#the-kubernetes-operator-metamodel","text":"This sections describes the key objects and concepts you need to understand before building operators. The diagram shows the key concepts and high level architecture, based on the Kubebuilder Architecture Concept Diagram Most of the concepts in the diagram are generic for all types of operators, no matter how they have been implemented. Some parts are specific to operators built with Golang, Operator SDK including Kubebuilder Architecture Concept Diagram and the Operator Lifecycle Manager Framework . Most of the concepts in the diagram are generic for all types of operators, no matter how they have been implemented. Some parts are specific to operators built with Golang , Operator SDK including Kubebuilder and the Operator Lifecycle Manager Framework . Below are some more details about the different objects. I have also added links to samples. The samples are part of a bigger sample, called operator-sample-go , which is available on GitHub that describes various operator patterns and best practises.","title":"The Kubernetes Operator Metamodel"},{"location":"overview-the-k8s-operator-metamodel/#operator","text":"The term \u2018Operator\u2019 or \u2018Kubernetes Operator\u2019 describes the mechanism to automate deployments and day 2 operations for software running on Kubernetes which implements the operator pattern . This pattern is used by Kubernetes internally as well as externally for custom resources. Operators contain custom resource definitions and business logic to manage these resources. The self contained operators are deployed as containers on Kubernetes. Usually there is one running operator instance per cluster. For production deployments the Operator Lifecycle Manager (OLM) provides functionality to deploy and operate the operators, for example to handle multiple versions. Operators are packaged in CSVs (cluster service versions). Samples: Operator Dockerfile Operator CSV Operator initialization in main.go","title":"Operator"},{"location":"overview-the-k8s-operator-metamodel/#api","text":"The term API is often used as synonym to custom resource definition. Custom resource definitions have schemas and potentially multiple versions. This allows managing resources declaratively in Kubernetes production environments. Custom resource definitions are identified by their group, version and resource name. One operator can contain multiple resource definitions. Samples: Sample resource Schema as Go struct Schema as yaml Versions","title":"API"},{"location":"overview-the-k8s-operator-metamodel/#manager","text":"The manager contains the business logic of the operator which knows how to deploy and manage custom resources. Additionally it comes with generic built in functionality to handle HA leader election, export metrics, handle webhook certs and broadcasts events. It also provides a client to access Kubernetes and a cache to improve efficiency. Sample: Manager creation in main.go","title":"Manager"},{"location":"overview-the-k8s-operator-metamodel/#controller","text":"The main responsibility of controllers is to synchronize the \u2018to be\u2019 states as defined in custom resources with the \u2018as is\u2019 states in Kubernetes clusters. This includes creations of new resources, updates to existing resources or deletions. This logic is implemented in the controllers\u2019 reconcile function. The reconciler doesn\u2019t use an imperative model to manage resources because of the nature of distributed Kubernetes systems and because of the long time it can take to change resources without blocking anything. Instead the reconciler is invoked over and over again until it signals that it\u2019s done. This is why reconcilers need to be idempotent. One controller manages one custom resource definition including all versions of it. The controller uses caches and Kubernetes clients and gets events via filters. Samples: Flow in Reconcile function Synchronization of resources Creations and updates of resources Definition of resources to watch","title":"Controller"},{"location":"overview-the-k8s-operator-metamodel/#webhooks","text":"With webhooks values of resources can be changed and conversions between different versions can be done. Samples: Initialization Validation Conversion","title":"Webhooks"},{"location":"overview-why-build-k8s-operators/","text":"Why you should build Kubernetes Operators \u00b6 There are multiple technologies to install and operate software in Kubernetes: kubectl, oc, Helm, Kustomize, CI/CD, GitOps, operators and more. As always the answer to the question when to use what is: \"It depends\". Consider the following reason to use operators: Top three Reasons to use Operators \u00b6 Automation of Day 2 Operations Autopilot and Self-Healing Predictive analytics Reusability of Software Hubs and Marketplaces Internal and external multi-tenancy Applications Leverage of the Kubernetes Community Ecosystem, Tools and Offerings Industry Standard 1) Automation of Day 2 Operations \u00b6 The main purpose of operators is to automate operations. While this sounds obvious, often there is a perception that operators are only another way to deploy software. The key value of operators is to make operations of software as seamless as possible. People often think of operators as the next best way after Software as as Service (SaaS) to easily manage software running in our own clusters. Kubernetes operators are able to automate expensive and error prone human operations. Features like autopilot and self-healing are great scenarios. For example databases can be archived automatically, software can automatically be scaled up and down and missing resources can be recreated when they have been deleted by mistake or in catastrophic events. It\u2019s even possible to predict failures before they happen and to take appropriate actions. These automations work for all types of workloads (stateful and stateless) and all types of Kubernetes resources (compute, storage, network). All business specific automation functionality can be bundled in one operator component rather than using a diverse set of tools. 2) Reusability of Software \u00b6 Another benefit of operators is the reusability of software. With operators, software and libraries can be bundled so that it can be used in different contexts. This approach is different from CI/CD pipelines where software is deployed more frequently specifically for one application. Software providers can expose their operators in various catalogs, hubs and marketplaces like OperatorHub.io, Red Hat Marketplace, the integrated OperatorHub in OpenShift, etc. This allows them to reach new markets and to promote their software. While catalogs like OperatorHub.io are primarily used to increase awareness, marketplaces like Red Hat Marketplace provide commercialization options. Additionally, operators can be utilized to share software within companies and to share software with specific clients. In this scenario operators can be published in internal catalogs or they can be directly deployed to clusters. 3) Leverage of the Kubernetes Community \u00b6 Kubernetes is the de-facto standard of how to run software in the cloud. There is a huge ecosystem supporting Kubernetes, providing samples, tools and commercial offerings and more. Operators leverage this community, since they are a natural and Kubernetes-native way to extend Kubernetes. Operators are not only used to manage third-party resources, but they are also used internally. Because operators base on the Kubernetes model, existing development tools, CLIs, monitoring tools, etc. can be used. Kubernetes provides proven and established industry standards of how to define, deploy and operate software. For example for production workloads a declarative approach is employed (e.g. GitOps ready), custom resources can be defined and standard CLIs and data formats (yaml, json) are used. It is not necessary to learn and utilize proprietary functionality.","title":"5.2 Why you should build Kubernetes Operators"},{"location":"overview-why-build-k8s-operators/#why-you-should-build-kubernetes-operators","text":"There are multiple technologies to install and operate software in Kubernetes: kubectl, oc, Helm, Kustomize, CI/CD, GitOps, operators and more. As always the answer to the question when to use what is: \"It depends\". Consider the following reason to use operators:","title":"Why you should build Kubernetes Operators"},{"location":"overview-why-build-k8s-operators/#top-three-reasons-to-use-operators","text":"Automation of Day 2 Operations Autopilot and Self-Healing Predictive analytics Reusability of Software Hubs and Marketplaces Internal and external multi-tenancy Applications Leverage of the Kubernetes Community Ecosystem, Tools and Offerings Industry Standard","title":"Top three Reasons to use Operators"},{"location":"overview-why-build-k8s-operators/#1-automation-of-day-2-operations","text":"The main purpose of operators is to automate operations. While this sounds obvious, often there is a perception that operators are only another way to deploy software. The key value of operators is to make operations of software as seamless as possible. People often think of operators as the next best way after Software as as Service (SaaS) to easily manage software running in our own clusters. Kubernetes operators are able to automate expensive and error prone human operations. Features like autopilot and self-healing are great scenarios. For example databases can be archived automatically, software can automatically be scaled up and down and missing resources can be recreated when they have been deleted by mistake or in catastrophic events. It\u2019s even possible to predict failures before they happen and to take appropriate actions. These automations work for all types of workloads (stateful and stateless) and all types of Kubernetes resources (compute, storage, network). All business specific automation functionality can be bundled in one operator component rather than using a diverse set of tools.","title":"1) Automation of Day 2 Operations"},{"location":"overview-why-build-k8s-operators/#2-reusability-of-software","text":"Another benefit of operators is the reusability of software. With operators, software and libraries can be bundled so that it can be used in different contexts. This approach is different from CI/CD pipelines where software is deployed more frequently specifically for one application. Software providers can expose their operators in various catalogs, hubs and marketplaces like OperatorHub.io, Red Hat Marketplace, the integrated OperatorHub in OpenShift, etc. This allows them to reach new markets and to promote their software. While catalogs like OperatorHub.io are primarily used to increase awareness, marketplaces like Red Hat Marketplace provide commercialization options. Additionally, operators can be utilized to share software within companies and to share software with specific clients. In this scenario operators can be published in internal catalogs or they can be directly deployed to clusters.","title":"2) Reusability of Software"},{"location":"overview-why-build-k8s-operators/#3-leverage-of-the-kubernetes-community","text":"Kubernetes is the de-facto standard of how to run software in the cloud. There is a huge ecosystem supporting Kubernetes, providing samples, tools and commercial offerings and more. Operators leverage this community, since they are a natural and Kubernetes-native way to extend Kubernetes. Operators are not only used to manage third-party resources, but they are also used internally. Because operators base on the Kubernetes model, existing development tools, CLIs, monitoring tools, etc. can be used. Kubernetes provides proven and established industry standards of how to define, deploy and operate software. For example for production workloads a declarative approach is employed (e.g. GitOps ready), custom resources can be defined and standard CLIs and data formats (yaml, json) are used. It is not necessary to learn and utilize proprietary functionality.","title":"3) Leverage of the Kubernetes Community"}]}